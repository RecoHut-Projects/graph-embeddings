{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6cx_OutcVmO"
      },
      "source": [
        "# Skip-gram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z94IZ8PMcN2R"
      },
      "source": [
        "Skip-Gram model seeks to optimize the word weight (embedding) matrix by correctly predicting context words, given a center word. In the other words, the model wants to maximize the probability of correctly predicting all context words at the same time, given a center word. Maximizing the probability of predicting context words leads to optimizing the weight matrix (θ) that best represents words in a vector space. Mathematically, it can be expressed as: $\\underset{\\theta}{\\text{argmax}} \\,\\, log\\ p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \\, \\theta)$\n",
        "\n",
        "### **CBOW and Skip-Gram**\n",
        "\n",
        "There are two models for Word2Vec: Continous Bag Of Words (CBOW) and Skip-Gram. While Skip-Gram model predicts context words given a center word, CBOW model predicts a center word given context words. According to Mikolov:\n",
        "\n",
        "Skip-gram: works well with small amount of the training data, represents well even rare words or phrases\n",
        "\n",
        "CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIOB1NsFcYrk"
      },
      "source": [
        "<p><center><figure><img src='_images/L128493_1.png'><figcaption>“The cat jumped over the puddle.” Given the central word “jumped,” the model will be able to predict the surrounding words: “The,” “cat,” “over,” “the,” “puddle.”</figcaption></figure></center></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bIhKb1cbCB"
      },
      "source": [
        "Skip-Gram model is a better choice most of the time due to its ability to predict infrequent words, but this comes at the price of increased computational cost. If training time is a big concern, and you have large enough data to overcome the issue of predicting infrequent words, CBOW model may be a more viable choice. The details of CBOW model won't be covered in this post.\n",
        "\n",
        "## References\n",
        "\n",
        "1. [https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling](https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling) `blog`\n",
        "2. [https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling) `blog`\n",
        "3. [Demystifying Neural Network in Skip-Gram Language Modeling](https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#Training:-Backward-Propagation)\n",
        "4. [Optimize Computational Efficiency of Skip-Gram with Negative Sampling](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "L128493_Skip_gram_model.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
