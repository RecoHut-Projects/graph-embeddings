{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKE1Op1tnbFI"
      },
      "source": [
        "# GATv2\n",
        "\n",
        "> How Attentive are Graph Attention Networks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrC8sgqnnwNd"
      },
      "source": [
        "### Abstract\n",
        "\n",
        "*Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GATs can only compute a restricted kind of attention where the ranking of attended nodes is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymAj__2gn5R_"
      },
      "source": [
        "GATv2s work on graph data similar to [GAT](https://nn.labml.ai/graphs/gat/index.html). A graph consists of nodes and edges connecting nodes. For example, in Cora dataset the nodes are research papers and the edges are citations that connect the papers.\n",
        "\n",
        "The GATv2 operator fixes the static attention problem of the standard [GAT](https://nn.labml.ai/graphs/gat/index.html). Static attention is when the attention to the key nodes has the same rank (order) for any query node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcqJjoIKnz0J"
      },
      "source": [
        "### References\n",
        "\n",
        "1. [https://github.com/tech-srl/how_attentive_are_gats](https://github.com/tech-srl/how_attentive_are_gats)\n",
        "2. [https://nn.labml.ai/graphs/gatv2/index.html](https://nn.labml.ai/graphs/gatv2/index.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "C110059_GATv2.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
