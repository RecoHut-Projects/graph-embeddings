{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfU6lv4ad6BG"
      },
      "source": [
        "# DeepWalk\n",
        "\n",
        "> Online Learning of Social Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJlCKb8TeEoo"
      },
      "source": [
        "The key idea is that the techniques which have been used to model natural language (where the symbol frequency follows a power law distribution (or Zipf ’s law)) can be repurposed to model community structure in networks. In this model, we generalizes the concept of language modeling to explore the graph through a stream of short random walks. These walks can be thought of short sentences and phrases in a special language. The direct analog is to estimate the likelihood of observing vertex $v_i$ given all the previous vertices visited so far in the random walk.\n",
        "\n",
        "$$P_r(v_i \\vert (v_1,v_2,...,v_{i-1}))$$\n",
        "\n",
        "But since our goal is to learn a latent representation, not only a probability distribution of node co-occurrences, so we use a mapping function $Φ: v ∈ V \\rightarrow \\mathbb{R}^{|V|×d}$. This mapping $Φ$ represents the latent social representation associated with each vertex v in the graph. The problem then, is to estimate the likelihood:\n",
        "\n",
        "$$Pr(v_i | (Φ(v_1), Φ(v_2), · · · , Φ(v_{i−1}))) $$\n",
        "\n",
        "Although as the walk length grows, computing this objective function becomes un-feasible, skip-gram concept in language modeling can be employed to solve this problem. First, instead of using the context to predict a missing word, it uses one word to predict the context. Secondly, the context is composed of the words appearing to right side of the given word as well as the left side. Finally, it removes the ordering constraint on the problem. Instead, the model is required to maximize the probability of any word appearing in the context without the knowledge of its offset from the given word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqm0WW54ePz7"
      },
      "source": [
        "<p><center><figure><img src='_images/C870584_1.png'><figcaption>DeepWalk algorithm</figcaption></figure></center></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvEIz-w3eMRA"
      },
      "source": [
        "Below picture explains the process clearly:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGhWUaAleQML"
      },
      "source": [
        "<p><center><figure><img src='_images/C870584_2.png'><figcaption>source - original paper.</figcaption></figure></center></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRehTbQSeEmJ"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [https://github.com/ninoxjy/graph-embedding](https://github.com/ninoxjy/graph-embedding) (2020) [Source code]\n",
        "2. [https://github.com/rforgione/deepwalk](https://github.com/rforgione/deepwalk) (2020) [Source code]\n",
        "3. [https://github.com/gen3111620/DeepWalk](https://github.com/gen3111620/DeepWalk) (2020) [Source code]\n",
        "4. [https://github.com/benedekrozemberczki/karateclub](https://github.com/benedekrozemberczki/karateclub) (2021) [Source code]\n",
        "5. [https://github.com/shenweichen/GraphEmbedding](https://github.com/shenweichen/GraphEmbedding) (2020) [Source code]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "C870584_DeepWalk.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
