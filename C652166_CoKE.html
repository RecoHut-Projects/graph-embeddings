
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CoKE &#8212; graph-embeddings</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="GAT" href="C047239_GAT.html" />
    <link rel="prev" title="Splitter" href="C779816_Splitter.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">graph-embeddings</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="US549467_Graph_embeddings.html">
   Graph embeddings
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L742313_Key_developments.html">
   Key developments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L991141_Random_walk.html">
   Random walk
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L128493_Skip_gram_model.html">
   Skip-gram model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L883045_Message_Passing_Neural_Networks.html">
   Message Passing Neural Networks
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Models
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="C870584_DeepWalk.html">
   DeepWalk
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C067906_LINE.html">
   LINE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C662128_SDNE.html">
   SDNE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C039027_Node2vec.html">
   Node2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C694808_Struc2Vec.html">
   Struc2Vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C779816_Splitter.html">
   Splitter
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   CoKE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C047239_GAT.html">
   GAT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C110059_GATv2.html">
   GATv2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C538124_BigGraph.html">
   BigGraph
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C503513_KHGT.html">
   KHGT
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T050508_Introduction_to_Networkx.html">
   Introduction to Networkx
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T528562_Graph_properties.html">
   Graph properties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T118630_Graph_Benchmarks.html">
   Graph Benchmarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T984528_Graph_encoder.html">
   Graph encoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T384270_DeepWalk_in_python.html">
   DeepWalk in python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T382881_DeepWalk_on_Karateclub.html">
   DeepWalk on Karateclub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T677598_DeepWalk_on_ML_100k.html">
   DeepWalk on ML-100k
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T862985_LINE_on_Wiki_network.html">
   LINE on Wiki network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T186367_Node2vec_from_scratch.html">
   Node2vec from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611050_Node2vec_in_PyTorch_Geometric_on_CORA_dataset.html">
   Node2vec in PyTorch Geometric on CORA dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T815556_Node2vec_on_Karateclub.html">
   Node2vec on Karateclub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T894941_Node2vec_on_ML_latest_in_Keras.html">
   Node2vec on ML-latest in Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T926278_Node2vec%2C_Edge2vec%2C_and_Graph2vec.html">
   Node2vec, Edge2vec, and Graph2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T401127_GEM_on_Karateclub.html">
   GEM on Karateclub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T481518_Struc2vec_on_airports_graph.html">
   Struc2vec on airports graph
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T516490_Learn_embeddings_using_Graph_Networks.html">
   Learn Embeddings using Graph Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T981902_Shallow_embedding_methods.html">
   Shallow embedding methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T189677_Graph_embeddings_using_SDNE.html">
   Graph embeddings using SDNE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T873032_Graph_embeddings_using_Convnet_Stellargraph.html">
   Graph embeddings using Convnet Stellargraph
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T674201_Graph_embeddings.html">
   Graph ML Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T457098_KHGT_knowledge_graph_embeddings.html">
   KHGT knowledge graph embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T403235_Contextualized_Knowledge_Graph_Embedding.html">
   Contextualized Knowledge Graph Embedding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T643979_Learning_Node_Representations_from_Multiple_Social_Contexts.html">
   SPLITTER: Learning Node Representations from Multiple Contexts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T874868_GAT_in_Tensorflow.x.html">
   GAT using Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T249662_GAT_in_PyTorch_on_CORA.html">
   GAT in PyTorch on CORA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T336765_FB15K_Graph_Embedding_Learning_in_PyTorch_Big_Graph.html">
   FB15K Graph Embedding Learning in PyTorch Big Graph
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T012114_SAGEConv_graph_embeddings_in_PyG.html">
   SAGEConv graph embeddings in PyG
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/C652166_CoKE.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/graph-embeddings/main?urlpath=lab/tree/docs/C652166_CoKE.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sparsh-ai/graph-embeddings/blob/main/docs/C652166_CoKE.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-formulation">
   Problem Formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-tasks">
   Training Tasks
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="coke">
<h1>CoKE<a class="headerlink" href="#coke" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><p>Contextualized Knowledge Graph Embedding</p>
</div></blockquote>
<p>CoKE takes a sequence as input and uses a Transformer encoder to obtain contextualized representations. The model is then trained by predicting a missing component in the sequence, based on these contextualized representations.</p>
<p>Unlike sequential left-to-right or right-to-left encoding strategies, Transformer uses a multi-head self-attention mechanism, which allows each element to attend to all elements in the sequence, and thus is more effective in context modeling.</p>
<p><center><figure><img src='_images/C652166_1.png'><figcaption>An example of Barack Obama, where the left subgraph shows his political role (dashed blue) and the right one his family role (solid orange).</figcaption></figure></center></p><p>Take relation HasPart as an example, which also presents contextualized meanings, e.g., composition-related as (Table, HasPart, Leg) and location-related as (Atlantics, HasPart, NewYorkBay) (Xiao et al., 2016). Learning entity and relation representations that could effectively capture their contextual meanings poses a new challenge to KG embedding.</p>
<div class="section" id="problem-formulation">
<h2>Problem Formulation<a class="headerlink" href="#problem-formulation" title="Permalink to this headline">¶</a></h2>
<p>We are given a KG composed of subject-relation-object triples {(s, r, o)}. Each triple indicates a relation r ∈ R between two entities s, o ∈ E, e.g., (Barack Obama, Has Child, Sasha Obama). Here, E is the entity vocabulary and R the relation set. These entities and relations form rich, varied graph contexts. Two types of graph contexts are considered here: edges and paths, both formalized as sequences composed of entities and relations.</p>
<ul class="simple">
<li><p>An edge s → r → o is a sequence formed by a triple, e.g., Barack Obama → Has Child → Sasha Obama. This is the basic unit of a KG, and also the simplest form of graph contexts.</p></li>
<li><p>A path <span class="math notranslate nohighlight">\(s → r_1 → · · · → r_k → o\)</span> is a sequence formed by a list of relations linking two entities, e.g., Barack Obama → Has Child (Sasha) −−−−−→ Lives In (US) −−→ Official Language → English. The length of a path is defined as the number of relations therein. The example above is a path of length 3. Edges can be viewed as special paths of length 1.</p></li>
</ul>
<p>Given a graph context, i.e., an edge or a path, we unify the input as a sequence <span class="math notranslate nohighlight">\(X = (x_1, x_2, · · · , x_n)\)</span>, where the first and last elements are entities from <span class="math notranslate nohighlight">\(E\)</span>, and the others in between are relations from <span class="math notranslate nohighlight">\(R\)</span>. For each element <span class="math notranslate nohighlight">\(x_i\)</span> in <span class="math notranslate nohighlight">\(X\)</span>, we construct its input representation as:</p>
<div class="math notranslate nohighlight">
\[h_i^0 = x_i^{ele} + x_i^{pos},\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i^{ele}\)</span> is the element embedding and <span class="math notranslate nohighlight">\(x_i^{pos}\)</span> the position embedding. The former is used to identify the current element, and the latter its position in the sequence. We allow an element embedding for each entity/relation in E ∪ R, and a position embedding for each position within length K. After constructing all input representations, we feed them into a stack of L successive Transformer encoders (Vaswani et al., 2017) to encode the sequence and obtain:</p>
<div class="math notranslate nohighlight">
\[h_i^l = Transformer(h_i^{l−1}),\ l= 1, 2, · · · , L\]</div>
<p>where <span class="math notranslate nohighlight">\(h_i^l\)</span> is the hidden state of xi after the <span class="math notranslate nohighlight">\(l\)</span>-th layer.</p>
</div>
<div class="section" id="training-tasks">
<h2>Training Tasks<a class="headerlink" href="#training-tasks" title="Permalink to this headline">¶</a></h2>
<p>To train the model, we design an entity prediction task, i.e., to predict a missing entity from a given graph context. This task amounts to single-hop or multi-hop question answering on KGs.</p>
<ul class="simple">
<li><p>Each edge s → r → o is associated with two training instances: ? → r → o and s → r →?. It is a single-hop question answering task, e.g., Barack Obama → Has Child →? is to answer “Who is the child of Barack Obama?”.</p></li>
<li><p>Each path <span class="math notranslate nohighlight">\(s → r_1 → · · · → r_k → o\)</span> is also associated with two training instances, one to predict s and the other to predict o. This is a multi-hop question answering task, e.g., Barack Obama → Has Child → Lives In → Official Language →? is to answer “What is the official language of the country where Barack Obama’s child lives in?”.</p></li>
</ul>
<p>This entity prediction task resembles the masked language model (MLM) task studied in (Devlin et al., 2019). But unlike MLM that randomly picks some input tokens to mask and predict, we restrict the masking and prediction solely to entities in a given edge/path, so as to create meaningful question answering instances. Moreover, many downstream tasks considered in the evaluation phase, e.g., link prediction and path query answering, can be formulated exactly in the same way as entity prediction, which avoids training-test discrepancy.</p>
<p><center><img src='_images/C652166_2.png'></center></p></div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sparsh-ai/graph-embeddings",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="C779816_Splitter.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Splitter</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="C047239_GAT.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">GAT</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>