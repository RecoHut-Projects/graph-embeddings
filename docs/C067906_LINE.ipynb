{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C067906 | LINE : Large-scale Information Network Embedding",
      "provenance": [],
      "authorship_tag": "ABX9TyNr4wnDVH6xwn4J0hGKIhgf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdZfj-xye8y7"
      },
      "source": [
        "# LINE\n",
        "\n",
        "> Large-scale Information Network Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCpXeDV_fQD5"
      },
      "source": [
        "## Abstract\n",
        "\n",
        "*This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the \"LINE,\" which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU9WMOrbfDkh"
      },
      "source": [
        "It defines loss functions to preserve the first-order or second-order proximity separately. After optimizing the loss functions, it concatenates these representations."
      ]
    }
  ]
}