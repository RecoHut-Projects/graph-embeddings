
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GAT in PyTorch on CORA &#8212; graph-embeddings</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="FB15K Graph Embedding Learning in PyTorch Big Graph" href="T336765_FB15K_Graph_Embedding_Learning_in_PyTorch_Big_Graph.html" />
    <link rel="prev" title="GAT using Tensorflow" href="T874868_GAT_in_Tensorflow.x.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">graph-embeddings</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="US549467_Graph_embeddings.html">
   Graph embeddings
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L742313_Key_developments.html">
   Key developments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L991141_Random_walk.html">
   Random walk
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L128493_Skip_gram_model.html">
   Skip-gram model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L883045_Message_Passing_Neural_Networks.html">
   Message Passing Neural Networks
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="C870584_DeepWalk.html">
   DeepWalk
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C067906_LINE.html">
   LINE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C662128_SDNE.html">
   SDNE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C039027_Node2vec.html">
   Node2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C694808_Struc2Vec.html">
   Struc2Vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C779816_Splitter.html">
   Splitter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C652166_CoKE.html">
   CoKE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C047239_GAT.html">
   GAT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C110059_GATv2.html">
   GATv2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C538124_BigGraph.html">
   BigGraph
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="C503513_KHGT.html">
   KHGT
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T050508_Introduction_to_Networkx.html">
   Introduction to Networkx
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T528562_Graph_properties.html">
   Graph properties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T118630_Graph_Benchmarks.html">
   Graph Benchmarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T984528_Graph_encoder.html">
   Graph encoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T384270_DeepWalk_in_python.html">
   DeepWalk in python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T382881_DeepWalk_on_Karateclub.html">
   DeepWalk on Karateclub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T677598_DeepWalk_on_ML_100k.html">
   DeepWalk on ML-100k
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T862985_LINE_on_Wiki_network.html">
   LINE on Wiki network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T186367_Node2vec_from_scratch.html">
   Node2vec from scratch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T611050_Node2vec_in_PyTorch_Geometric_on_CORA_dataset.html">
   Node2vec in PyTorch Geometric on CORA dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T815556_Node2vec_on_Karateclub.html">
   Node2vec on Karateclub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T894941_Node2vec_on_ML_latest_in_Keras.html">
   Node2vec on ML-latest in Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T926278_Node2vec_Edge2vec_and_Graph2vec.html">
   Node2vec, Edge2vec, and Graph2vec
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T401127_GEM_on_Karateclub.html">
   GEM on Karateclub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T481518_Struc2vec_on_airports_graph.html">
   Struc2vec on airports graph
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T516490_Learn_embeddings_using_Graph_Networks.html">
   Learn Embeddings using Graph Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T981902_Shallow_embedding_methods.html">
   Shallow embedding methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T189677_Graph_embeddings_using_SDNE.html">
   Graph embeddings using SDNE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T873032_Graph_embeddings_using_Convnet_Stellargraph.html">
   Graph embeddings using Convnet Stellargraph
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T674201_Graph_embeddings.html">
   Graph ML Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T457098_KHGT_knowledge_graph_embeddings.html">
   KHGT knowledge graph embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T403235_Contextualized_Knowledge_Graph_Embedding.html">
   Contextualized Knowledge Graph Embedding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T643979_Learning_Node_Representations_from_Multiple_Social_Contexts.html">
   SPLITTER: Learning Node Representations from Multiple Contexts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T874868_GAT_in_Tensorflow.x.html">
   GAT using Tensorflow
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   GAT in PyTorch on CORA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T336765_FB15K_Graph_Embedding_Learning_in_PyTorch_Big_Graph.html">
   FB15K Graph Embedding Learning in PyTorch Big Graph
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T012114_SAGEConv_graph_embeddings_in_PyG.html">
   SAGEConv graph embeddings in PyG
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/T249662_GAT_in_PyTorch_on_CORA.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/graph-embeddings/main?urlpath=lab/tree/docs/T249662_GAT_in_PyTorch_on_CORA.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sparsh-ai/graph-embeddings/blob/main/docs/T249662_GAT_in_PyTorch_on_CORA.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#utils">
   Utils
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#download-the-data">
     Download the data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load">
     Load
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-your-data">
     Visualizing your data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#understanding-gat-s-inner-workings">
   Understanding GAT’s inner workings
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-gat-classification-on-cora">
   Training GAT (Classification on Cora!)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visualizing-gat">
   Visualizing GAT
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-gat-s-embeddings-using-t-sne">
     Visualizing GAT’s embeddings using t-SNE
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-neighborhood-attention">
     Visualizing neighborhood attention
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-entropy-histograms">
     Visualizing entropy histograms
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="gat-in-pytorch-on-cora">
<h1>GAT in PyTorch on CORA<a class="headerlink" href="#gat-in-pytorch-on-cora" title="Permalink to this headline">¶</a></h1>
<div class="section" id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">reload_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!apt-get install libcairo2-dev
!pip install cairocffi
!pip install python-igraph
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">igraph</span> <span class="k">as</span> <span class="nn">ig</span>

<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="utils">
<h2>Utils<a class="headerlink" href="#utils" title="Permalink to this headline">¶</a></h2>
<p><strong>Contains constants needed for data loading and visualization</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">enum</span>


<span class="c1"># Supported datasets - only Cora in this notebook</span>
<span class="k">class</span> <span class="nc">DatasetType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">CORA</span> <span class="o">=</span> <span class="mi">0</span>

    
<span class="c1"># Networkx is not precisely made with drawing as its main feature but I experimented with it a bit</span>
<span class="k">class</span> <span class="nc">GraphVisualizationTool</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">NETWORKX</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">IGRAPH</span> <span class="o">=</span> <span class="mi">1</span>


<span class="c1"># We&#39;ll be dumping and reading the data from this directory</span>
<span class="n">CORA_PATH</span> <span class="o">=</span> <span class="s1">&#39;/content&#39;</span>

<span class="c1">#</span>
<span class="c1"># Cora specific constants</span>
<span class="c1">#</span>

<span class="c1"># Thomas Kipf et al. first used this split in GCN paper and later Petar Veličković et al. in GAT paper</span>
<span class="n">CORA_TRAIN_RANGE</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">140</span><span class="p">]</span>  <span class="c1"># we&#39;re using the first 140 nodes as the training nodes</span>
<span class="n">CORA_VAL_RANGE</span> <span class="o">=</span> <span class="p">[</span><span class="mi">140</span><span class="p">,</span> <span class="mi">140</span><span class="o">+</span><span class="mi">500</span><span class="p">]</span>
<span class="n">CORA_TEST_RANGE</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1708</span><span class="p">,</span> <span class="mi">1708</span><span class="o">+</span><span class="mi">1000</span><span class="p">]</span>
<span class="n">CORA_NUM_INPUT_FEATURES</span> <span class="o">=</span> <span class="mi">1433</span>
<span class="n">CORA_NUM_CLASSES</span> <span class="o">=</span> <span class="mi">7</span>

<span class="c1"># Used whenever we need to visualzie points from different classes (t-SNE, CORA visualization)</span>
<span class="n">cora_label_to_color_map</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="s2">&quot;yellow&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">:</span> <span class="s2">&quot;pink&quot;</span><span class="p">,</span> <span class="mi">6</span><span class="p">:</span> <span class="s2">&quot;gray&quot;</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p><strong>simple functions for loading/saving Pickle files</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># All Cora data is stored as pickle</span>
<span class="k">def</span> <span class="nf">pickle_read</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">pickle_save</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">file</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle</span><span class="o">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="download-the-data">
<h3>Download the data<a class="headerlink" href="#download-the-data" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!wget -q --show-progress https://github.com/gordicaleksa/pytorch-GAT/raw/main/data/cora/adjacency_list.dict
!wget -q --show-progress https://github.com/gordicaleksa/pytorch-GAT/raw/main/data/cora/node_features.csr
!wget -q --show-progress https://github.com/gordicaleksa/pytorch-GAT/raw/main/data/cora/node_labels.npy
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>adjacency_list.dict 100%[===================&gt;]  58.44K  --.-KB/s    in 0.01s   
node_features.csr   100%[===================&gt;] 395.44K  --.-KB/s    in 0.07s   
node_labels.npy     100%[===================&gt;]  21.30K  --.-KB/s    in 0.001s  
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="load">
<h3>Load<a class="headerlink" href="#load" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># We&#39;ll pass the training config dictionary a bit later</span>
<span class="k">def</span> <span class="nf">load_graph_data</span><span class="p">(</span><span class="n">training_config</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">dataset_name</span> <span class="o">=</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;dataset_name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">should_visualize</span> <span class="o">=</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;should_visualize&#39;</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">dataset_name</span> <span class="o">==</span> <span class="n">DatasetType</span><span class="o">.</span><span class="n">CORA</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>

        <span class="c1"># shape = (N, FIN), where N is the number of nodes and FIN is the number of input features</span>
        <span class="n">node_features_csr</span> <span class="o">=</span> <span class="n">pickle_read</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CORA_PATH</span><span class="p">,</span> <span class="s1">&#39;node_features.csr&#39;</span><span class="p">))</span>
        <span class="c1"># shape = (N, 1)</span>
        <span class="n">node_labels_npy</span> <span class="o">=</span> <span class="n">pickle_read</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CORA_PATH</span><span class="p">,</span> <span class="s1">&#39;node_labels.npy&#39;</span><span class="p">))</span>
        <span class="c1"># shape = (N, number of neighboring nodes) &lt;- this is a dictionary not a matrix!</span>
        <span class="n">adjacency_list_dict</span> <span class="o">=</span> <span class="n">pickle_read</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CORA_PATH</span><span class="p">,</span> <span class="s1">&#39;adjacency_list.dict&#39;</span><span class="p">))</span>

        <span class="c1"># Normalize the features (helps with training)</span>
        <span class="n">node_features_csr</span> <span class="o">=</span> <span class="n">normalize_features_sparse</span><span class="p">(</span><span class="n">node_features_csr</span><span class="p">)</span>
        <span class="n">num_of_nodes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">node_labels_npy</span><span class="p">)</span>

        <span class="c1"># shape = (2, E), where E is the number of edges, and 2 for source and target nodes. Basically edge index</span>
        <span class="c1"># contains tuples of the format S-&gt;T, e.g. 0-&gt;3 means that node with id 0 points to a node with id 3.</span>
        <span class="n">topology</span> <span class="o">=</span> <span class="n">build_edge_index</span><span class="p">(</span><span class="n">adjacency_list_dict</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">,</span> <span class="n">add_self_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Note: topology is just a fancy way of naming the graph structure data </span>
        <span class="c1"># (aside from edge index it could be in the form of an adjacency matrix)</span>

        <span class="k">if</span> <span class="n">should_visualize</span><span class="p">:</span>  <span class="c1"># network analysis and graph drawing</span>
            <span class="n">plot_in_out_degree_distributions</span><span class="p">(</span><span class="n">topology</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">)</span>  <span class="c1"># we&#39;ll define these in a second</span>
            <span class="n">visualize_graph</span><span class="p">(</span><span class="n">topology</span><span class="p">,</span> <span class="n">node_labels_npy</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">)</span>

        <span class="c1"># Convert to dense PyTorch tensors</span>

        <span class="c1"># Needs to be long int type because later functions like PyTorch&#39;s index_select expect it</span>
        <span class="n">topology</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">topology</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">node_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">node_labels_npy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># Cross entropy expects a long int</span>
        <span class="n">node_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">node_features_csr</span><span class="o">.</span><span class="n">todense</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Indices that help us extract nodes that belong to the train/val and test splits</span>
        <span class="n">train_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">CORA_TRAIN_RANGE</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">CORA_TRAIN_RANGE</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">val_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">CORA_VAL_RANGE</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">CORA_VAL_RANGE</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">test_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">CORA_TEST_RANGE</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">CORA_TEST_RANGE</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">node_features</span><span class="p">,</span> <span class="n">node_labels</span><span class="p">,</span> <span class="n">topology</span><span class="p">,</span> <span class="n">train_indices</span><span class="p">,</span> <span class="n">val_indices</span><span class="p">,</span> <span class="n">test_indices</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s1"> not yet supported.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Nice, there are 2 more functions that I’ve used that we’re yet to define. First let’s see how we do <strong>feature normalization</strong> on Cora:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">normalize_features_sparse</span><span class="p">(</span><span class="n">node_features_sparse</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">node_features_sparse</span><span class="p">),</span> <span class="sa">f</span><span class="s1">&#39;Expected a sparse matrix, got </span><span class="si">{</span><span class="n">node_features_sparse</span><span class="si">}</span><span class="s1">.&#39;</span>

    <span class="c1"># Instead of dividing (like in normalize_features_dense()) we do multiplication with inverse sum of features.</span>
    <span class="c1"># Modern hardware (GPUs, TPUs, ASICs) is optimized for fast matrix multiplications! ^^ (* &gt;&gt; /)</span>
    <span class="c1"># shape = (N, FIN) -&gt; (N, 1), where N number of nodes and FIN number of input features</span>
    <span class="n">node_features_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">node_features_sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># sum features for every node feature vector</span>

    <span class="c1"># Make an inverse (remember * by 1/x is better (faster) then / by x)</span>
    <span class="c1"># shape = (N, 1) -&gt; (N)</span>
    <span class="n">node_features_inv_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">node_features_sum</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

    <span class="c1"># Again certain sums will be 0 so 1/0 will give us inf so we replace those by 1 which is a neutral element for mul</span>
    <span class="n">node_features_inv_sum</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">node_features_inv_sum</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">1.</span>

    <span class="c1"># Create a diagonal matrix whose values on the diagonal come from node_features_inv_sum</span>
    <span class="n">diagonal_inv_features_sum_matrix</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">diags</span><span class="p">(</span><span class="n">node_features_inv_sum</span><span class="p">)</span>

    <span class="c1"># We return the normalized features.</span>
    <span class="k">return</span> <span class="n">diagonal_inv_features_sum_matrix</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">node_features_sparse</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>It’s basically making Cora’s binary node feature vectors sum up to 1. Example if we had <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">1]</span></code> (Cora’s feature vectors are longer as we’ll soon see but let’s take this one for the time being), it will get transformed into <code class="docutils literal notranslate"><span class="pre">[0.33,</span> <span class="pre">0,</span> <span class="pre">0.33,</span> <span class="pre">0,</span> <span class="pre">0.33]</span></code>. Simple as that. It’s always harder to understand the actual implementation, but conceptually, it’s a piece of cake.</p>
<p>That out of the way let’s build up that <strong>edge index</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_edge_index</span><span class="p">(</span><span class="n">adjacency_list_dict</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">,</span> <span class="n">add_self_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">source_nodes_ids</span><span class="p">,</span> <span class="n">target_nodes_ids</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">seen_edges</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">src_node</span><span class="p">,</span> <span class="n">neighboring_nodes</span> <span class="ow">in</span> <span class="n">adjacency_list_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">trg_node</span> <span class="ow">in</span> <span class="n">neighboring_nodes</span><span class="p">:</span>
            <span class="c1"># if this edge hasn&#39;t been seen so far we add it to the edge index (coalescing - removing duplicates)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">src_node</span><span class="p">,</span> <span class="n">trg_node</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_edges</span><span class="p">:</span>  <span class="c1"># it&#39;d be easy to explicitly remove self-edges (Cora has none..)</span>
                <span class="n">source_nodes_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">src_node</span><span class="p">)</span>
                <span class="n">target_nodes_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trg_node</span><span class="p">)</span>

                <span class="n">seen_edges</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">src_node</span><span class="p">,</span> <span class="n">trg_node</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">add_self_edges</span><span class="p">:</span>
        <span class="n">source_nodes_ids</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_of_nodes</span><span class="p">))</span>
        <span class="n">target_nodes_ids</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_of_nodes</span><span class="p">))</span>

    <span class="c1"># shape = (2, E), where E is the number of edges in the graph</span>
    <span class="n">edge_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">row_stack</span><span class="p">((</span><span class="n">source_nodes_ids</span><span class="p">,</span> <span class="n">target_nodes_ids</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">edge_index</span>
</pre></div>
</div>
</div>
</div>
<p>This one should be fairly simple - we just accumulate the edges in this format: <br/>
[[0, 1], [2, 2], …] where [s, t] tuple basically defines an edge where node <code class="docutils literal notranslate"><span class="pre">s</span></code> (source) points to node <code class="docutils literal notranslate"><span class="pre">t</span></code> (target).</p>
<p>Other popular format is the <strong>adjacency matrix</strong> - but those take up way more memory (O(N^2) to be precise, compare that to O(E) for the edge index structure).</p>
<p>Nice, finally let’s try and load it. We should also analyze the shapes - that’s always a good idea.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s just define dummy visualization functions for now - just to stop Python interpreter from complaining!</span>
<span class="c1"># We&#39;ll define them in a moment, properly, I swear.</span>

<span class="k">def</span> <span class="nf">plot_in_out_degree_distributions</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="k">def</span> <span class="nf">visualize_graph</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>  <span class="c1"># checking whether you have a GPU</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;dataset_name&#39;</span><span class="p">:</span> <span class="n">DatasetType</span><span class="o">.</span><span class="n">CORA</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
    <span class="s1">&#39;should_visualize&#39;</span><span class="p">:</span> <span class="kc">False</span>
<span class="p">}</span>

<span class="n">node_features</span><span class="p">,</span> <span class="n">node_labels</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">train_indices</span><span class="p">,</span> <span class="n">val_indices</span><span class="p">,</span> <span class="n">test_indices</span> <span class="o">=</span> <span class="n">load_graph_data</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">node_features</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">node_features</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">node_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">node_labels</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">edge_index</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">edge_index</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">val_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">val_indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_indices</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2708, 1433]) torch.float32
torch.Size([2708]) torch.int64
torch.Size([2, 13264]) torch.int64
torch.Size([140]) torch.int64
torch.Size([500]) torch.int64
torch.Size([1000]) torch.int64
</pre></div>
</div>
</div>
</div>
<p>Nice! Analyzing the shapes we see the following:</p>
<ol class="simple">
<li><p>Cora has 2708 nodes</p></li>
<li><p>Each node has 1433 features (check out <a class="reference external" href="https://github.com/gordicaleksa/pytorch-GAT/blob/main/utils/data_loading.py">data_loading.py</a> for much more detail)</p></li>
<li><p>We have 13264 edges! (including the self edges)</p></li>
<li><p>We have 140 training nodes</p></li>
<li><p>We have 500 val nodes</p></li>
<li><p>We have 1000 test nodes</p></li>
</ol>
<p>Additionally almost all of the data is of int 64 type. Why? Well it’s a constraint that PyTorch is imposing upon us.
The loss function <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code> and <code class="docutils literal notranslate"><span class="pre">index_select</span></code> functions require torch.long (i.e. 64 bit integer) - that’s it.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">node_labels</span></code> is int64 because of <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code></p></li>
<li><p>other vars are int64 because of <code class="docutils literal notranslate"><span class="pre">index_select</span></code></p></li>
</ul>
<p>On the “side note”, it’s always a <strong>good idea to test your code as you’re progressing.</strong></p>
<p>Data loading is completely orthogonal to the rest of this notebook so we can test it, standalone, and make sure the shapes and datatypes make sense. I use this strategy while developing projects like this one (and in general).</p>
<p>I start with data, I add the loading functionality, I add some visualizations and only then do I usually start developing the deep learning model itself.</p>
<p>Visualizations are a huge bonus, so let’s develop them.</p>
</div>
<div class="section" id="visualizing-your-data">
<h3>Visualizing your data<a class="headerlink" href="#visualizing-your-data" title="Permalink to this headline">¶</a></h3>
<p>Let’s start by understanding the degree distribution of nodes in Cora - i.e. how many input/output edges do nodes have, a certain measure of connectedness of the graph.</p>
<p>Run the following cell:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_in_out_degree_distributions</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note: It would be easy to do various kinds of powerful network analysis using igraph/networkx, etc.</span>
<span class="sd">        I chose to explicitly calculate only the node degree statistics here, but you can go much further if needed and</span>
<span class="sd">        calculate the graph diameter, number of triangles and many other concepts from the network analysis field.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">edge_index</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">),</span> <span class="sa">f</span><span class="s1">&#39;Expected NumPy array got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">edge_index</span><span class="p">)</span><span class="si">}</span><span class="s1">.&#39;</span>

    <span class="c1"># Store each node&#39;s input and output degree (they&#39;re the same for undirected graphs such as Cora)</span>
    <span class="n">in_degrees</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_of_nodes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
    <span class="n">out_degrees</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_of_nodes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

    <span class="c1"># Edge index shape = (2, E), the first row contains the source nodes, the second one target/sink nodes</span>
    <span class="c1"># Note on terminology: source nodes point to target/sink nodes</span>
    <span class="n">num_of_edges</span> <span class="o">=</span> <span class="n">edge_index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">cnt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_of_edges</span><span class="p">):</span>
        <span class="n">source_node_id</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">cnt</span><span class="p">]</span>
        <span class="n">target_node_id</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">cnt</span><span class="p">]</span>

        <span class="n">out_degrees</span><span class="p">[</span><span class="n">source_node_id</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># source node points towards some other node -&gt; increment its out degree</span>
        <span class="n">in_degrees</span><span class="p">[</span><span class="n">target_node_id</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># similarly here</span>

    <span class="n">hist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">out_degrees</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">out_degree</span> <span class="ow">in</span> <span class="n">out_degrees</span><span class="p">:</span>
        <span class="n">hist</span><span class="p">[</span><span class="n">out_degree</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># otherwise plots are really small in Jupyter Notebook</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">311</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">in_degrees</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;node id&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;in-degree count&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input degree for different node ids&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">312</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out_degrees</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;node id&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;out-degree count&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Out degree for different node ids&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">313</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;node degree&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;# nodes for a given out-degree&#39;</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Node out-degree distribution for </span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s1"> dataset&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">hist</span><span class="p">),</span> <span class="mf">5.0</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Brilliant, let’s now visualize Cora’s degree distributions!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_of_nodes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">node_labels</span><span class="p">)</span>
<span class="n">plot_in_out_degree_distributions</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;dataset_name&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_25_0.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_25_0.png" />
</div>
</div>
<p>You can immediately notice a couple of things:</p>
<ul class="simple">
<li><p>The top 2 plots are the same, because we treat Cora as an undirected graph (even though it should naturally be modeled as a directed graph)</p></li>
<li><p>Certain nodes have a huge number of edges (the peak in the middle) but most nodes have far less edges</p></li>
<li><p>The third plot nicely visualizes this in the form of a histogram - most nodes have only <code class="docutils literal notranslate"><span class="pre">2-5</span></code> edges (hence the peak on the leftmost side)</p></li>
</ul>
<p>Ok, we’re starting to get some valuable insight into Cora, let’s continue further and literally visualize/see Cora.</p>
<p>The following cell will plot Cora, run it. (<em>whispers: run it</em>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Check out this blog for available graph visualization tools:</span>
<span class="sd">    https://towardsdatascience.com/large-graph-visualization-tools-and-approaches-2b8758a1cd59</span>

<span class="sd">Basically depending on how big your graph is there may be better drawing tools than igraph.</span>

<span class="sd">Note: I unfortunatelly had to flatten this function since igraph is having some problems with Jupyter Notebook,</span>
<span class="sd">we&#39;ll only call it here so it&#39;s fine!</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="n">dataset_name</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;dataset_name&#39;</span><span class="p">]</span>
<span class="n">visualization_tool</span><span class="o">=</span><span class="n">GraphVisualizationTool</span><span class="o">.</span><span class="n">IGRAPH</span>

<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">edge_index_np</span> <span class="o">=</span> <span class="n">edge_index</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node_labels</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">node_labels_np</span> <span class="o">=</span> <span class="n">node_labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">num_of_nodes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">node_labels_np</span><span class="p">)</span>
<span class="n">edge_index_tuples</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">edge_index_np</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">edge_index_np</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]))</span>  <span class="c1"># igraph requires this format</span>

<span class="c1"># Construct the igraph graph</span>
<span class="n">ig_graph</span> <span class="o">=</span> <span class="n">ig</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">ig_graph</span><span class="o">.</span><span class="n">add_vertices</span><span class="p">(</span><span class="n">num_of_nodes</span><span class="p">)</span>
<span class="n">ig_graph</span><span class="o">.</span><span class="n">add_edges</span><span class="p">(</span><span class="n">edge_index_tuples</span><span class="p">)</span>

<span class="c1"># Prepare the visualization settings dictionary</span>
<span class="n">visual_style</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># Defines the size of the plot and margins</span>
<span class="c1"># go berserk here try (3000, 3000) it looks amazing in Jupyter!!! (you&#39;ll have to adjust the vertex_size though!)</span>
<span class="n">visual_style</span><span class="p">[</span><span class="s2">&quot;bbox&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">700</span><span class="p">,</span> <span class="mi">700</span><span class="p">)</span>
<span class="n">visual_style</span><span class="p">[</span><span class="s2">&quot;margin&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># I&#39;ve chosen the edge thickness such that it&#39;s proportional to the number of shortest paths (geodesics)</span>
<span class="c1"># that go through a certain edge in our graph (edge_betweenness function, a simple ad hoc heuristic)</span>

<span class="c1"># line1: I use log otherwise some edges will be too thick and others not visible at all</span>
<span class="c1"># edge_betweeness returns &lt; 1 for certain edges that&#39;s why I use clip as log would be negative for those edges</span>
<span class="c1"># line2: Normalize so that the thickest edge is 1 otherwise edges appear too thick on the chart</span>
<span class="c1"># line3: The idea here is to make the strongest edge stay stronger than others, 6 just worked, don&#39;t dwell on it</span>

<span class="n">edge_weights_raw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">ig_graph</span><span class="o">.</span><span class="n">edge_betweenness</span><span class="p">())</span><span class="o">+</span><span class="mf">1e-16</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">edge_weights_raw_normalized</span> <span class="o">=</span> <span class="n">edge_weights_raw</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">edge_weights_raw</span><span class="p">)</span>
<span class="n">edge_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">**</span><span class="mi">6</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">edge_weights_raw_normalized</span><span class="p">]</span>
<span class="n">visual_style</span><span class="p">[</span><span class="s2">&quot;edge_width&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">edge_weights</span>

<span class="c1"># A simple heuristic for vertex size. Size ~ (degree / 4) (it gave nice results I tried log and sqrt as well)</span>
<span class="n">visual_style</span><span class="p">[</span><span class="s2">&quot;vertex_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">deg</span> <span class="o">/</span> <span class="mi">4</span> <span class="k">for</span> <span class="n">deg</span> <span class="ow">in</span> <span class="n">ig_graph</span><span class="o">.</span><span class="n">degree</span><span class="p">()]</span>

<span class="c1"># This is the only part that&#39;s Cora specific as Cora has 7 labels</span>
<span class="k">if</span> <span class="n">dataset_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="n">DatasetType</span><span class="o">.</span><span class="n">CORA</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
    <span class="n">visual_style</span><span class="p">[</span><span class="s2">&quot;vertex_color&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">cora_label_to_color_map</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">node_labels_np</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Feel free to add custom color scheme for your specific dataset. Using igraph default coloring.&#39;</span><span class="p">)</span>

<span class="c1"># Set the layout - the way the graph is presented on a 2D chart. Graph drawing is a subfield for itself!</span>
<span class="c1"># I used &quot;Kamada Kawai&quot; a force-directed method, this family of methods are based on physical system simulation.</span>
<span class="c1"># (layout_drl also gave nice results for Cora)</span>
<span class="n">visual_style</span><span class="p">[</span><span class="s2">&quot;layout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ig_graph</span><span class="o">.</span><span class="n">layout_kamada_kawai</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Plotting results ... (it may take couple of seconds).&#39;</span><span class="p">)</span>
<span class="n">ig</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ig_graph</span><span class="p">,</span> <span class="o">**</span><span class="n">visual_style</span><span class="p">)</span>

<span class="c1"># This website has got some awesome visualizations check it out:</span>
<span class="c1"># http://networkrepository.com/graphvis.php?d=./data/gsm50/labeled/cora.edges</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output hidden; open in https://colab.research.google.com to view.
</pre></div>
</div>
</div>
</div>
<p>I don’t know about you, but I think this one is beautiful!</p>
<p>Try running it with <code class="docutils literal notranslate"><span class="pre">visual_style[&quot;bbox&quot;]</span></code> set to <code class="docutils literal notranslate"><span class="pre">(3000,</span> <span class="pre">3000)</span></code> and use <code class="docutils literal notranslate"><span class="pre">/</span> <span class="pre">2</span></code> in <code class="docutils literal notranslate"><span class="pre">vertex_size</span></code> and you’ll get a huuuge and awesome plot (<code class="docutils literal notranslate"><span class="pre">C</span></code> handles the plotting behind igraph so it’s pretty fast on my machine at least - with some minor lagging when you scroll over it).</p>
<p>Ok we’re done with visualizations and understanding our data. This is a huge milestone, so tap yourself on the back. 🏆🎂🎵</p>
<p>We have the level 2 unlocked (the GAT model 🦄). 😍</p>
<p>And now, let’s understand the model!</p>
</div>
</div>
<div class="section" id="understanding-gat-s-inner-workings">
<h2>Understanding GAT’s inner workings<a class="headerlink" href="#understanding-gat-s-inner-workings" title="Permalink to this headline">¶</a></h2>
<p>First let’s create a high level class where we’ll build up <code class="docutils literal notranslate"><span class="pre">GAT</span></code> from <code class="docutils literal notranslate"><span class="pre">GatLayer</span></code> objects.</p>
<p>It basically just stacks the layers into a nn.Sequential object and additionally since nn.Sequential expects a single input (and it has a single output) I just pack the data (features, edge index) into a tuple - <em>pure syntactic sugar</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>


<span class="k">class</span> <span class="nc">GAT</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The most interesting and hardest implementation is implementation #3.</span>
<span class="sd">    Imp1 and imp2 differ in subtle details but are basically the same thing.</span>

<span class="sd">    So I&#39;ll focus on imp #3 in this notebook.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_of_layers</span><span class="p">,</span> <span class="n">num_heads_per_layer</span><span class="p">,</span> <span class="n">num_features_per_layer</span><span class="p">,</span> <span class="n">add_skip_connection</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">log_attention_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">num_of_layers</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_heads_per_layer</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_features_per_layer</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Enter valid arch params.&#39;</span>

        <span class="n">num_heads_per_layer</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num_heads_per_layer</span>  <span class="c1"># trick - so that I can nicely create GAT layers below</span>

        <span class="n">gat_layers</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># collect GAT layers</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_of_layers</span><span class="p">):</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="n">GATLayer</span><span class="p">(</span>
                <span class="n">num_in_features</span><span class="o">=</span><span class="n">num_features_per_layer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_heads_per_layer</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>  <span class="c1"># consequence of concatenation</span>
                <span class="n">num_out_features</span><span class="o">=</span><span class="n">num_features_per_layer</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">num_of_heads</span><span class="o">=</span><span class="n">num_heads_per_layer</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">concat</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_of_layers</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># last GAT layer does mean avg, the others do concat</span>
                <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_of_layers</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># last layer just outputs raw scores</span>
                <span class="n">dropout_prob</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                <span class="n">add_skip_connection</span><span class="o">=</span><span class="n">add_skip_connection</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">log_attention_weights</span><span class="o">=</span><span class="n">log_attention_weights</span>
            <span class="p">)</span>
            <span class="n">gat_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gat_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="n">gat_layers</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:</span>
    <span class="c1"># https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gat_net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now for the fun part let’s define the layer.</p>
<p>I really don’t think that I can explain it any better, using words, than you taking your time to digest the code and the comments.</p>
<p>Also make sure to check out <a class="reference external" href="https://www.youtube.com/watch?v=uFLeKkXWq2c">my video on GAT</a> before you start losing time trying to figure it out “from scratch”. It’s always good to have some theoretical background at your hand.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GATLayer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric</span>

<span class="sd">    But, it&#39;s hopefully much more readable! (and of similar performance)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># We&#39;ll use these constants in many functions so just extracting them here as member fields</span>
    <span class="n">src_nodes_dim</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># position of source nodes in edge index</span>
    <span class="n">trg_nodes_dim</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># position of target nodes in edge index</span>

    <span class="c1"># These may change in the inductive setting - leaving it like this for now (not future proof)</span>
    <span class="n">nodes_dim</span> <span class="o">=</span> <span class="mi">0</span>      <span class="c1"># node dimension (axis is maybe a more familiar term nodes_dim is the position of &quot;N&quot; in tensor)</span>
    <span class="n">head_dim</span> <span class="o">=</span> <span class="mi">1</span>       <span class="c1"># attention head dim</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_in_features</span><span class="p">,</span> <span class="n">num_out_features</span><span class="p">,</span> <span class="n">num_of_heads</span><span class="p">,</span> <span class="n">concat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">(),</span>
                 <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">add_skip_connection</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">log_attention_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_of_heads</span> <span class="o">=</span> <span class="n">num_of_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_out_features</span> <span class="o">=</span> <span class="n">num_out_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">concat</span>  <span class="c1"># whether we should concatenate or average the attention heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_skip_connection</span> <span class="o">=</span> <span class="n">add_skip_connection</span>

        <span class="c1">#</span>
        <span class="c1"># Trainable weights: linear projection matrix (denoted as &quot;W&quot; in the paper), attention target/source</span>
        <span class="c1"># (denoted as &quot;a&quot; in the paper) and bias (not mentioned in the paper but present in the official GAT repo)</span>
        <span class="c1">#</span>

        <span class="c1"># You can treat this one matrix as num_of_heads independent W matrices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_in_features</span><span class="p">,</span> <span class="n">num_of_heads</span> <span class="o">*</span> <span class="n">num_out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># After we concatenate target node (node i) and source node (node j) we apply the &quot;additive&quot; scoring function</span>
        <span class="c1"># which gives us un-normalized score &quot;e&quot;. Here we split the &quot;a&quot; vector - but the semantics remain the same.</span>
        <span class="c1"># Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with &quot;a&quot;</span>
        <span class="c1"># we instead do a dot product between x and &quot;a_left&quot; and y and &quot;a_right&quot; and we sum them up</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scoring_fn_target</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_of_heads</span><span class="p">,</span> <span class="n">num_out_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scoring_fn_source</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_of_heads</span><span class="p">,</span> <span class="n">num_out_features</span><span class="p">))</span>

        <span class="c1"># Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">and</span> <span class="n">concat</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">num_of_heads</span> <span class="o">*</span> <span class="n">num_out_features</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">bias</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">concat</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">num_out_features</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">add_skip_connection</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">skip_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_in_features</span><span class="p">,</span> <span class="n">num_of_heads</span> <span class="o">*</span> <span class="n">num_out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;skip_proj&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1">#</span>
        <span class="c1"># End of trainable weights</span>
        <span class="c1">#</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">leakyReLU</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>  <span class="c1"># using 0.2 as in the paper, no need to expose every setting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="c1"># Probably not the nicest design but I use the same module in 3 locations, before/after features projection</span>
        <span class="c1"># and for attention coefficients. Functionality-wise it&#39;s the same as using independent modules.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_attention_weights</span> <span class="o">=</span> <span class="n">log_attention_weights</span>  <span class="c1"># whether we should log the attention weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># for later visualization purposes, I cache the weights here</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_params</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="c1">#</span>
        <span class="c1"># Step 1: Linear Projection + regularization</span>
        <span class="c1">#</span>

        <span class="n">in_nodes_features</span><span class="p">,</span> <span class="n">edge_index</span> <span class="o">=</span> <span class="n">data</span>  <span class="c1"># unpack data</span>
        <span class="n">num_of_nodes</span> <span class="o">=</span> <span class="n">in_nodes_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes_dim</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">edge_index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Expected edge index with shape=(2,E) got </span><span class="si">{</span><span class="n">edge_index</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span>

        <span class="c1"># shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node</span>
        <span class="c1"># We apply the dropout to all of the input node features (as mentioned in the paper)</span>
        <span class="c1"># Note: for Cora features are already super sparse so it&#39;s questionable how much this actually helps</span>
        <span class="n">in_nodes_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">in_nodes_features</span><span class="p">)</span>

        <span class="c1"># shape = (N, FIN) * (FIN, NH*FOUT) -&gt; (N, NH, FOUT) where NH - number of heads, FOUT - num of output features</span>
        <span class="c1"># We project the input node features into NH independent output features (one for each attention head)</span>
        <span class="n">nodes_features_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_proj</span><span class="p">(</span><span class="n">in_nodes_features</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_of_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_out_features</span><span class="p">)</span>

        <span class="n">nodes_features_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">nodes_features_proj</span><span class="p">)</span>  <span class="c1"># in the official GAT imp they did dropout here as well</span>

        <span class="c1">#</span>
        <span class="c1"># Step 2: Edge attention calculation</span>
        <span class="c1">#</span>

        <span class="c1"># Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)</span>
        <span class="c1"># shape = (N, NH, FOUT) * (1, NH, FOUT) -&gt; (N, NH, 1) -&gt; (N, NH) because sum squeezes the last dimension</span>
        <span class="c1"># Optimization note: torch.sum() is as performant as .sum() in my experiments</span>
        <span class="n">scores_source</span> <span class="o">=</span> <span class="p">(</span><span class="n">nodes_features_proj</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scoring_fn_source</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">nodes_features_proj</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scoring_fn_target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all</span>
        <span class="c1"># the possible combinations of scores we just prepare those that will actually be used and those are defined</span>
        <span class="c1"># by the edge index.</span>
        <span class="c1"># scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph</span>
        <span class="n">scores_source_lifted</span><span class="p">,</span> <span class="n">scores_target_lifted</span><span class="p">,</span> <span class="n">nodes_features_proj_lifted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lift</span><span class="p">(</span><span class="n">scores_source</span><span class="p">,</span> <span class="n">scores_target</span><span class="p">,</span> <span class="n">nodes_features_proj</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
        <span class="n">scores_per_edge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leakyReLU</span><span class="p">(</span><span class="n">scores_source_lifted</span> <span class="o">+</span> <span class="n">scores_target_lifted</span><span class="p">)</span>

        <span class="c1"># shape = (E, NH, 1)</span>
        <span class="n">attentions_per_edge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neighborhood_aware_softmax</span><span class="p">(</span><span class="n">scores_per_edge</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">trg_nodes_dim</span><span class="p">],</span> <span class="n">num_of_nodes</span><span class="p">)</span>
        <span class="c1"># Add stochasticity to neighborhood aggregation</span>
        <span class="n">attentions_per_edge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attentions_per_edge</span><span class="p">)</span>

        <span class="c1">#</span>
        <span class="c1"># Step 3: Neighborhood aggregation</span>
        <span class="c1">#</span>

        <span class="c1"># Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul</span>
        <span class="c1"># shape = (E, NH, FOUT) * (E, NH, 1) -&gt; (E, NH, FOUT), 1 gets broadcast into FOUT</span>
        <span class="n">nodes_features_proj_lifted_weighted</span> <span class="o">=</span> <span class="n">nodes_features_proj_lifted</span> <span class="o">*</span> <span class="n">attentions_per_edge</span>

        <span class="c1"># This part sums up weighted and projected neighborhood feature vectors for every target node</span>
        <span class="c1"># shape = (N, NH, FOUT)</span>
        <span class="n">out_nodes_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregate_neighbors</span><span class="p">(</span><span class="n">nodes_features_proj_lifted_weighted</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">in_nodes_features</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">)</span>

        <span class="c1">#</span>
        <span class="c1"># Step 4: Residual/skip connections, concat and bias</span>
        <span class="c1">#</span>

        <span class="n">out_nodes_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_concat_bias</span><span class="p">(</span><span class="n">attentions_per_edge</span><span class="p">,</span> <span class="n">in_nodes_features</span><span class="p">,</span> <span class="n">out_nodes_features</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">out_nodes_features</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>

    <span class="c1">#</span>
    <span class="c1"># Helper functions (without comments there is very little code so don&#39;t be scared!)</span>
    <span class="c1">#</span>

    <span class="k">def</span> <span class="nf">neighborhood_aware_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores_per_edge</span><span class="p">,</span> <span class="n">trg_index</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.</span>
<span class="sd">        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take</span>
<span class="sd">        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3</span>
<span class="sd">        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)</span>
<span class="sd">        (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3</span>
<span class="sd">         i.e. for this neighborhood we don&#39;t care about other edge scores that include nodes 4 and 5.</span>

<span class="sd">        Note:</span>
<span class="sd">        Subtracting the max value from logits doesn&#39;t change the end result but it improves the numerical stability</span>
<span class="sd">        and it&#39;s a fairly common &quot;trick&quot; used in pretty much every deep learning framework.</span>
<span class="sd">        Check out this link for more details:</span>

<span class="sd">        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Calculate the numerator. Make logits &lt;= 0 so that e^logit &lt;= 1 (this will improve the numerical stability)</span>
        <span class="n">scores_per_edge</span> <span class="o">=</span> <span class="n">scores_per_edge</span> <span class="o">-</span> <span class="n">scores_per_edge</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">exp_scores_per_edge</span> <span class="o">=</span> <span class="n">scores_per_edge</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>  <span class="c1"># softmax</span>

        <span class="c1"># Calculate the denominator. shape = (E, NH)</span>
        <span class="n">neigborhood_aware_denominator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum_edge_scores_neighborhood_aware</span><span class="p">(</span><span class="n">exp_scores_per_edge</span><span class="p">,</span> <span class="n">trg_index</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">)</span>

        <span class="c1"># 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the</span>
        <span class="c1"># possibility of the computer rounding a very small number all the way to 0.</span>
        <span class="n">attentions_per_edge</span> <span class="o">=</span> <span class="n">exp_scores_per_edge</span> <span class="o">/</span> <span class="p">(</span><span class="n">neigborhood_aware_denominator</span> <span class="o">+</span> <span class="mf">1e-16</span><span class="p">)</span>

        <span class="c1"># shape = (E, NH) -&gt; (E, NH, 1) so that we can do element-wise multiplication with projected node features</span>
        <span class="k">return</span> <span class="n">attentions_per_edge</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sum_edge_scores_neighborhood_aware</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exp_scores_per_edge</span><span class="p">,</span> <span class="n">trg_index</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">):</span>
        <span class="c1"># The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -&gt; (E, NH)</span>
        <span class="n">trg_index_broadcasted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">explicit_broadcast</span><span class="p">(</span><span class="n">trg_index</span><span class="p">,</span> <span class="n">exp_scores_per_edge</span><span class="p">)</span>

        <span class="c1"># shape = (N, NH), where N is the number of nodes and NH the number of attention heads</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">exp_scores_per_edge</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># convert to list otherwise assignment is not possible</span>
        <span class="n">size</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_of_nodes</span>
        <span class="n">neighborhood_sums</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">exp_scores_per_edge</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">exp_scores_per_edge</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the</span>
        <span class="c1"># target index)</span>
        <span class="n">neighborhood_sums</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes_dim</span><span class="p">,</span> <span class="n">trg_index_broadcasted</span><span class="p">,</span> <span class="n">exp_scores_per_edge</span><span class="p">)</span>

        <span class="c1"># Expand again so that we can use it as a softmax denominator. e.g. node i&#39;s sum will be copied to</span>
        <span class="c1"># all the locations where the source nodes pointed to i (as dictated by the target index)</span>
        <span class="c1"># shape = (N, NH) -&gt; (E, NH)</span>
        <span class="k">return</span> <span class="n">neighborhood_sums</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes_dim</span><span class="p">,</span> <span class="n">trg_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">aggregate_neighbors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nodes_features_proj_lifted_weighted</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">in_nodes_features</span><span class="p">,</span> <span class="n">num_of_nodes</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nodes_features_proj_lifted_weighted</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># convert to list otherwise assignment is not possible</span>
        <span class="n">size</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes_dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_of_nodes</span>  <span class="c1"># shape = (N, NH, FOUT)</span>
        <span class="n">out_nodes_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">in_nodes_features</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">in_nodes_features</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># shape = (E) -&gt; (E, NH, FOUT)</span>
        <span class="n">trg_index_broadcasted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">explicit_broadcast</span><span class="p">(</span><span class="n">edge_index</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">trg_nodes_dim</span><span class="p">],</span> <span class="n">nodes_features_proj_lifted_weighted</span><span class="p">)</span>
        <span class="c1"># aggregation step - we accumulate projected, weighted node features for all the attention heads</span>
        <span class="c1"># shape = (E, NH, FOUT) -&gt; (N, NH, FOUT)</span>
        <span class="n">out_nodes_features</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes_dim</span><span class="p">,</span> <span class="n">trg_index_broadcasted</span><span class="p">,</span> <span class="n">nodes_features_proj_lifted_weighted</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out_nodes_features</span>

    <span class="k">def</span> <span class="nf">lift</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores_source</span><span class="p">,</span> <span class="n">scores_target</span><span class="p">,</span> <span class="n">nodes_features_matrix_proj</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Lifts i.e. duplicates certain vectors depending on the edge index.</span>
<span class="sd">        One of the tensor dims goes from N -&gt; E (that&#39;s where the &quot;lift&quot; comes from).</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">src_nodes_index</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">src_nodes_dim</span><span class="p">]</span>
        <span class="n">trg_nodes_index</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">trg_nodes_dim</span><span class="p">]</span>

        <span class="c1"># Using index_select is faster than &quot;normal&quot; indexing (scores_source[src_nodes_index]) in PyTorch!</span>
        <span class="n">scores_source</span> <span class="o">=</span> <span class="n">scores_source</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes_dim</span><span class="p">,</span> <span class="n">src_nodes_index</span><span class="p">)</span>
        <span class="n">scores_target</span> <span class="o">=</span> <span class="n">scores_target</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes_dim</span><span class="p">,</span> <span class="n">trg_nodes_index</span><span class="p">)</span>
        <span class="n">nodes_features_matrix_proj_lifted</span> <span class="o">=</span> <span class="n">nodes_features_matrix_proj</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nodes_dim</span><span class="p">,</span> <span class="n">src_nodes_index</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scores_source</span><span class="p">,</span> <span class="n">scores_target</span><span class="p">,</span> <span class="n">nodes_features_matrix_proj_lifted</span>

    <span class="k">def</span> <span class="nf">explicit_broadcast</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">this</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="c1"># Append singleton dimensions until this.dim() == other.dim()</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">this</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">other</span><span class="o">.</span><span class="n">dim</span><span class="p">()):</span>
            <span class="n">this</span> <span class="o">=</span> <span class="n">this</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Explicitly expand so that shapes are the same</span>
        <span class="k">return</span> <span class="n">this</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The reason we&#39;re using Glorot (aka Xavier uniform) initialization is because it&#39;s a default TF initialization:</span>
<span class="sd">            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow</span>

<span class="sd">        The original repo was developed in TensorFlow (TF) and they used the default initialization.</span>
<span class="sd">        Feel free to experiment - there may be better initializations depending on your problem.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring_fn_target</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring_fn_source</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">skip_concat_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_coefficients</span><span class="p">,</span> <span class="n">in_nodes_features</span><span class="p">,</span> <span class="n">out_nodes_features</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_attention_weights</span><span class="p">:</span>  <span class="c1"># potentially log for later visualization in playground.py</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_coefficients</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_skip_connection</span><span class="p">:</span>  <span class="c1"># add skip or residual connection</span>
            <span class="k">if</span> <span class="n">out_nodes_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">in_nodes_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>  <span class="c1"># if FIN == FOUT</span>
                <span class="c1"># unsqueeze does this: (N, FIN) -&gt; (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH</span>
                <span class="c1"># thus we&#39;re basically copying input vectors NH times and adding to processed vectors</span>
                <span class="n">out_nodes_features</span> <span class="o">+=</span> <span class="n">in_nodes_features</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># FIN != FOUT so we need to project input feature vectors into dimension that can be added to output</span>
                <span class="c1"># feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.</span>
                <span class="n">out_nodes_features</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_proj</span><span class="p">(</span><span class="n">in_nodes_features</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_of_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_out_features</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">:</span>
            <span class="c1"># shape = (N, NH, FOUT) -&gt; (N, NH*FOUT)</span>
            <span class="n">out_nodes_features</span> <span class="o">=</span> <span class="n">out_nodes_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_of_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_out_features</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># shape = (N, NH, FOUT) -&gt; (N, FOUT)</span>
            <span class="n">out_nodes_features</span> <span class="o">=</span> <span class="n">out_nodes_features</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">out_nodes_features</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

        <span class="k">return</span> <span class="n">out_nodes_features</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">out_nodes_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The main idea that leads to huge savings is that we calculate the scores only for the nodes that will actually be used and not for every imaginable combination (that would be valid only in a fully-connected graph).</p>
<p>Once we compute the <code class="docutils literal notranslate"><span class="pre">&quot;left&quot;</span></code> scores and the <code class="docutils literal notranslate"><span class="pre">&quot;right&quot;</span></code> scores, we “lift” them up using the edge index. That way
if the edge <code class="docutils literal notranslate"><span class="pre">1-&gt;2</span></code> is not present in the graph we won’t have those score pairs in our data structure.</p>
<p>After adding lifted “left” and “right” (or maybe a better naming would be source and target) scores we do smart <code class="docutils literal notranslate"><span class="pre">neighborhood-aware</span> <span class="pre">softmax</span></code> - so that the semantics of GAT is respected. After doing the <code class="docutils literal notranslate"><span class="pre">scatter</span> <span class="pre">add</span></code> (which you should take your time to understand and go through the docs) we can combine the projected feature vectors, and voilà, we got ourselves a fully-blown GAT layer.</p>
<hr class="docutils" />
<p>Take your time and <strong>be patient</strong>! Especially if you’re new to GNNs.</p>
<p>I didn’t learn all of this in 1 day, it takes time for the knowledge to sink in. You’ll get there as well! ❤️ (if you’re not already there 😜)</p>
<p>Having said that, we’ve got the level 3 unlocked (model training 💪). 😍</p>
<p>We have the data 📜 ready, we have the GAT model 🦄 ready, let’s start training this beast! 💪</p>
</div>
<div class="section" id="training-gat-classification-on-cora">
<h2>Training GAT (Classification on Cora!)<a class="headerlink" href="#training-gat-classification-on-cora" title="Permalink to this headline">¶</a></h2>
<p>Phew, well the hardest part is behind us. Let’s know create a simple training loop where the goal is to learn to classify Cora nodes.</p>
<p>But first let’s define some relevant constants.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>


<span class="c1"># 3 different model training/eval phases used in train.py</span>
<span class="k">class</span> <span class="nc">LoopPhase</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">TRAIN</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">VAL</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">TEST</span> <span class="o">=</span> <span class="mi">2</span>

    
<span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">()</span>  <span class="c1"># (tensorboard) writer will output to ./runs/ directory by default</span>


<span class="c1"># Global vars used for early stopping. After some number of epochs (as defined by the patience_period var) without any</span>
<span class="c1"># improvement on the validation dataset (measured via accuracy metric), we&#39;ll break out from the training loop.</span>
<span class="n">BEST_VAL_ACC</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">BEST_VAL_LOSS</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">PATIENCE_CNT</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">BINARIES_PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="s1">&#39;models&#39;</span><span class="p">,</span> <span class="s1">&#39;binaries&#39;</span><span class="p">)</span>
<span class="n">CHECKPOINTS_PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="s1">&#39;models&#39;</span><span class="p">,</span> <span class="s1">&#39;checkpoints&#39;</span><span class="p">)</span>

<span class="c1"># Make sure these exist as the rest of the code assumes it</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">BINARIES_PATH</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">CHECKPOINTS_PATH</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Also, let’s define a couple of functions that will be useful while training the model.</p>
<p>The training state contains a lot of useful <code class="docutils literal notranslate"><span class="pre">metadata</span></code> which we can later use. You can imagine that saving the test accuracy of your model is important, especially when you’re training your models on a cloud - it makes the organization so much better.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>  <span class="c1"># regex</span>


<span class="k">def</span> <span class="nf">get_training_state</span><span class="p">(</span><span class="n">training_config</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">training_state</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1"># Training details</span>
        <span class="s2">&quot;dataset_name&quot;</span><span class="p">:</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;dataset_name&#39;</span><span class="p">],</span>
        <span class="s2">&quot;num_of_epochs&quot;</span><span class="p">:</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;num_of_epochs&#39;</span><span class="p">],</span>
        <span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">],</span>

        <span class="c1"># Model structure</span>
        <span class="s2">&quot;num_of_layers&quot;</span><span class="p">:</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;num_of_layers&#39;</span><span class="p">],</span>
        <span class="s2">&quot;num_heads_per_layer&quot;</span><span class="p">:</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;num_heads_per_layer&#39;</span><span class="p">],</span>
        <span class="s2">&quot;num_features_per_layer&quot;</span><span class="p">:</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;num_features_per_layer&#39;</span><span class="p">],</span>
        <span class="s2">&quot;add_skip_connection&quot;</span><span class="p">:</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;add_skip_connection&#39;</span><span class="p">],</span>
        <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">],</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">training_config</span><span class="p">[</span><span class="s1">&#39;dropout&#39;</span><span class="p">],</span>

        <span class="c1"># Model state</span>
        <span class="s2">&quot;state_dict&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">training_state</span>


<span class="k">def</span> <span class="nf">print_model_metadata</span><span class="p">(</span><span class="n">training_state</span><span class="p">):</span>
    <span class="n">header</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="si">{</span><span class="s2">&quot;*&quot;</span><span class="o">*</span><span class="mi">5</span><span class="si">}</span><span class="s1"> Model training metadata: </span><span class="si">{</span><span class="s2">&quot;*&quot;</span><span class="o">*</span><span class="mi">5</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">header</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">training_state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s1">&#39;state_dict&#39;</span><span class="p">:</span>  <span class="c1"># don&#39;t print state_dict just a bunch of numbers...</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="s2">&quot;*&quot;</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">header</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>


<span class="c1"># This one makes sure we don&#39;t overwrite the valuable model binaries (feel free to ignore - not crucial to GAT method)</span>
<span class="k">def</span> <span class="nf">get_available_binary_name</span><span class="p">():</span>
    <span class="n">prefix</span> <span class="o">=</span> <span class="s1">&#39;gat&#39;</span>

    <span class="k">def</span> <span class="nf">valid_binary_name</span><span class="p">(</span><span class="n">binary_name</span><span class="p">):</span>
        <span class="c1"># First time you see raw f-string? Don&#39;t worry the only trick is to double the brackets.</span>
        <span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s1">_[0-9]</span><span class="se">{{</span><span class="s1">6</span><span class="se">}}</span><span class="s1">\.pth&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">binary_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="c1"># Just list the existing binaries so that we don&#39;t overwrite them but write to a new one</span>
    <span class="n">valid_binary_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">valid_binary_name</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">BINARIES_PATH</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_binary_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">last_binary_name</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">valid_binary_names</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">new_suffix</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">last_binary_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">6</span><span class="p">:])</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># increment by 1</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">new_suffix</span><span class="p">)</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="si">}</span><span class="s1">.pth&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s1">_000000.pth&#39;</span>
</pre></div>
</div>
</div>
</div>
<p>Nice, now <code class="docutils literal notranslate"><span class="pre">argparse</span></code> is just a nice way to <strong>organize</strong> your program settings:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>


<span class="k">def</span> <span class="nf">get_training_args</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>

    <span class="c1"># Training related</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--num_of_epochs&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of training epochs&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--patience_period&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;number of epochs with no improvement on val before terminating&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;model learning rate&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">5e-3</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--weight_decay&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;L2 regularization on model weights&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--should_test&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;should test the model on the test dataset?&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Dataset related</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--dataset_name&quot;</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="n">el</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">DatasetType</span><span class="p">],</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;dataset to use for training&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">DatasetType</span><span class="o">.</span><span class="n">CORA</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--should_visualize&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;should visualize the dataset?&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Logging/debugging/checkpoint related (helps a lot with experimentation)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--enable_tensorboard&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;enable tensorboard logging&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--console_log_freq&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;log to output console (epoch) freq (None for no logging)&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--checkpoint_freq&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;checkpoint model saving (epoch) freq (None for no logging)&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

    <span class="c1"># Model architecture related - this is the architecture as defined in the official paper (for Cora classification)</span>
    <span class="n">gat_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;num_of_layers&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># GNNs, contrary to CNNs, are often shallow (it ultimately depends on the graph properties)</span>
        <span class="s2">&quot;num_heads_per_layer&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="s2">&quot;num_features_per_layer&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">CORA_NUM_INPUT_FEATURES</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">CORA_NUM_CLASSES</span><span class="p">],</span>
        <span class="s2">&quot;add_skip_connection&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># hurts perf on Cora</span>
        <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># result is not so sensitive to bias</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>  <span class="c1"># result is sensitive to dropout</span>
    <span class="p">}</span>

    <span class="c1"># Wrapping training configuration into a dictionary</span>
    <span class="n">training_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="nb">vars</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="n">training_config</span><span class="p">[</span><span class="n">arg</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">arg</span><span class="p">)</span>

    <span class="c1"># Add additional config information</span>
    <span class="n">training_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">gat_config</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">training_config</span>
</pre></div>
</div>
</div>
</div>
<p>Now for the juicy part. 🍪🎅</p>
<p>Here, we organize, high-level, everything we need for training GAT. Just combining the components we already learned.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>


<span class="k">def</span> <span class="nf">train_gat</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">BEST_VAL_ACC</span><span class="p">,</span> <span class="n">BEST_VAL_LOSS</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>  <span class="c1"># checking whether you have a GPU, I hope so!</span>

    <span class="c1"># Step 1: load the graph data</span>
    <span class="n">node_features</span><span class="p">,</span> <span class="n">node_labels</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">train_indices</span><span class="p">,</span> <span class="n">val_indices</span><span class="p">,</span> <span class="n">test_indices</span> <span class="o">=</span> <span class="n">load_graph_data</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

    <span class="c1"># Step 2: prepare the model</span>
    <span class="n">gat</span> <span class="o">=</span> <span class="n">GAT</span><span class="p">(</span>
        <span class="n">num_of_layers</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;num_of_layers&#39;</span><span class="p">],</span>
        <span class="n">num_heads_per_layer</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;num_heads_per_layer&#39;</span><span class="p">],</span>
        <span class="n">num_features_per_layer</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;num_features_per_layer&#39;</span><span class="p">],</span>
        <span class="n">add_skip_connection</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;add_skip_connection&#39;</span><span class="p">],</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">],</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;dropout&#39;</span><span class="p">],</span>
        <span class="n">log_attention_weights</span><span class="o">=</span><span class="kc">False</span>  <span class="c1"># no need to store attentions, used only in playground.py while visualizing</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Step 3: Prepare other training related utilities (loss &amp; optimizer and decorator function)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">gat</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">],</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

    <span class="c1"># THIS IS THE CORE OF THE TRAINING (we&#39;ll define it in a minute)</span>
    <span class="c1"># The decorator function makes things cleaner since there is a lot of redundancy between the train and val loops</span>
    <span class="n">main_loop</span> <span class="o">=</span> <span class="n">get_main_loop</span><span class="p">(</span>
        <span class="n">config</span><span class="p">,</span>
        <span class="n">gat</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">node_features</span><span class="p">,</span>
        <span class="n">node_labels</span><span class="p">,</span>
        <span class="n">edge_index</span><span class="p">,</span>
        <span class="n">train_indices</span><span class="p">,</span>
        <span class="n">val_indices</span><span class="p">,</span>
        <span class="n">test_indices</span><span class="p">,</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;patience_period&#39;</span><span class="p">],</span>
        <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span>

    <span class="n">BEST_VAL_ACC</span><span class="p">,</span> <span class="n">BEST_VAL_LOSS</span><span class="p">,</span> <span class="n">PATIENCE_CNT</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># reset vars used for early stopping</span>

    <span class="c1"># Step 4: Start the training procedure</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;num_of_epochs&#39;</span><span class="p">]):</span>
        <span class="c1"># Training loop</span>
        <span class="n">main_loop</span><span class="p">(</span><span class="n">phase</span><span class="o">=</span><span class="n">LoopPhase</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

        <span class="c1"># Validation loop</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">main_loop</span><span class="p">(</span><span class="n">phase</span><span class="o">=</span><span class="n">LoopPhase</span><span class="o">.</span><span class="n">VAL</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># &quot;patience has run out&quot; exception :O</span>
                <span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                <span class="k">break</span>  <span class="c1"># break out from the training loop</span>

    <span class="c1"># Step 5: Potentially test your model</span>
    <span class="c1"># Don&#39;t overfit to the test dataset - only when you&#39;ve fine-tuned your model on the validation dataset should you</span>
    <span class="c1"># report your final loss and accuracy on the test dataset. Friends don&#39;t let friends overfit to the test data. &lt;3</span>
    <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;should_test&#39;</span><span class="p">]:</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">main_loop</span><span class="p">(</span><span class="n">phase</span><span class="o">=</span><span class="n">LoopPhase</span><span class="o">.</span><span class="n">TEST</span><span class="p">)</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_acc</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test accuracy = </span><span class="si">{</span><span class="n">test_acc</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="c1"># Save the latest GAT in the binaries directory</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">get_training_state</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">gat</span><span class="p">),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">BINARIES_PATH</span><span class="p">,</span> <span class="n">get_available_binary_name</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<p>🎉🎉🎉</p>
<p>Now for the core part of the training - the main loop, as I’ve dubbed it.</p>
<p>I’ve organized it like this so that I don’t have to copy/paste bunch of the same code for train/val/test loops.</p>
<p><strong>Friends don’t let friends copy/paste (unless it’s from the Stack Overflow)</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simple decorator function so that I don&#39;t have to pass arguments that don&#39;t change from epoch to epoch</span>
<span class="k">def</span> <span class="nf">get_main_loop</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">gat</span><span class="p">,</span> <span class="n">cross_entropy_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">node_features</span><span class="p">,</span> <span class="n">node_labels</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">train_indices</span><span class="p">,</span> <span class="n">val_indices</span><span class="p">,</span> <span class="n">test_indices</span><span class="p">,</span> <span class="n">patience_period</span><span class="p">,</span> <span class="n">time_start</span><span class="p">):</span>

    <span class="n">node_dim</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># this will likely change as soon as I add an inductive example (Cora is transductive)</span>

    <span class="n">train_labels</span> <span class="o">=</span> <span class="n">node_labels</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">node_dim</span><span class="p">,</span> <span class="n">train_indices</span><span class="p">)</span>
    <span class="n">val_labels</span> <span class="o">=</span> <span class="n">node_labels</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">node_dim</span><span class="p">,</span> <span class="n">val_indices</span><span class="p">)</span>
    <span class="n">test_labels</span> <span class="o">=</span> <span class="n">node_labels</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">node_dim</span><span class="p">,</span> <span class="n">test_indices</span><span class="p">)</span>

    <span class="c1"># node_features shape = (N, FIN), edge_index shape = (2, E)</span>
    <span class="n">graph_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">node_features</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>  <span class="c1"># I pack data into tuples because GAT uses nn.Sequential which requires it</span>

    <span class="k">def</span> <span class="nf">get_node_indices</span><span class="p">(</span><span class="n">phase</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">LoopPhase</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">train_indices</span>
        <span class="k">elif</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">LoopPhase</span><span class="o">.</span><span class="n">VAL</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">val_indices</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">test_indices</span>

    <span class="k">def</span> <span class="nf">get_node_labels</span><span class="p">(</span><span class="n">phase</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">LoopPhase</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">train_labels</span>
        <span class="k">elif</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">LoopPhase</span><span class="o">.</span><span class="n">VAL</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">val_labels</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">test_labels</span>

    <span class="k">def</span> <span class="nf">main_loop</span><span class="p">(</span><span class="n">phase</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">global</span> <span class="n">BEST_VAL_ACC</span><span class="p">,</span> <span class="n">BEST_VAL_LOSS</span><span class="p">,</span> <span class="n">PATIENCE_CNT</span><span class="p">,</span> <span class="n">writer</span>

        <span class="c1"># Certain modules behave differently depending on whether we&#39;re training the model or not.</span>
        <span class="c1"># e.g. nn.Dropout - we only want to drop model weights during the training.</span>
        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">LoopPhase</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span>
            <span class="n">gat</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gat</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="n">node_indices</span> <span class="o">=</span> <span class="n">get_node_indices</span><span class="p">(</span><span class="n">phase</span><span class="p">)</span>
        <span class="n">gt_node_labels</span> <span class="o">=</span> <span class="n">get_node_labels</span><span class="p">(</span><span class="n">phase</span><span class="p">)</span>  <span class="c1"># gt stands for ground truth</span>

        <span class="c1"># Do a forwards pass and extract only the relevant node scores (train/val or test ones)</span>
        <span class="c1"># Note: [0] just extracts the node_features part of the data (index 1 contains the edge_index)</span>
        <span class="c1"># shape = (N, C) where N is the number of nodes in the split (train/val/test) and C is the number of classes</span>
        <span class="n">nodes_unnormalized_scores</span> <span class="o">=</span> <span class="n">gat</span><span class="p">(</span><span class="n">graph_data</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">node_dim</span><span class="p">,</span> <span class="n">node_indices</span><span class="p">)</span>

        <span class="c1"># Example: let&#39;s take an output for a single node on Cora - it&#39;s a vector of size 7 and it contains unnormalized</span>
        <span class="c1"># scores like: V = [-1.393,  3.0765, -2.4445,  9.6219,  2.1658, -5.5243, -4.6247]</span>
        <span class="c1"># What PyTorch&#39;s cross entropy loss does is for every such vector it first applies a softmax, and so we&#39;ll</span>
        <span class="c1"># have the V transformed into: [1.6421e-05, 1.4338e-03, 5.7378e-06, 0.99797, 5.7673e-04, 2.6376e-07, 6.4848e-07]</span>
        <span class="c1"># secondly, whatever the correct class is (say it&#39;s 3), it will then take the element at position 3,</span>
        <span class="c1"># 0.99797 in this case, and the loss will be -log(0.99797). It does this for every node and applies a mean.</span>
        <span class="c1"># You can see that as the probability of the correct class for most nodes approaches 1 we get to 0 loss! &lt;3</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">nodes_unnormalized_scores</span><span class="p">,</span> <span class="n">gt_node_labels</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">LoopPhase</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># clean the trainable weights gradients in the computational graph (.grad fields)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># compute the gradients for every trainable weight in the computational graph</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># apply the gradients to weights</span>

        <span class="c1"># Finds the index of maximum (unnormalized) score for every node and that&#39;s the class prediction for that node.</span>
        <span class="c1"># Compare those to true (ground truth) labels and find the fraction of correct predictions -&gt; accuracy metric.</span>
        <span class="n">class_predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">nodes_unnormalized_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">class_predictions</span><span class="p">,</span> <span class="n">gt_node_labels</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">gt_node_labels</span><span class="p">)</span>

        <span class="c1">#</span>
        <span class="c1"># Logging</span>
        <span class="c1">#</span>

        <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">LoopPhase</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span>
            <span class="c1"># Log metrics</span>
            <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;enable_tensorboard&#39;</span><span class="p">]:</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;training_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">epoch</span><span class="p">)</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;training_acc&#39;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>

            <span class="c1"># Save model checkpoint</span>
            <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;checkpoint_freq&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;checkpoint_freq&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">ckpt_model_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;gat_ckpt_epoch_</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">.pth&quot;</span>
                <span class="n">config</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">get_training_state</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">gat</span><span class="p">),</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">CHECKPOINTS_PATH</span><span class="p">,</span> <span class="n">ckpt_model_name</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">LoopPhase</span><span class="o">.</span><span class="n">VAL</span><span class="p">:</span>
            <span class="c1"># Log metrics</span>
            <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;enable_tensorboard&#39;</span><span class="p">]:</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">epoch</span><span class="p">)</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;val_acc&#39;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>

            <span class="c1"># Log to console</span>
            <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;console_log_freq&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;console_log_freq&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;GAT training: time elapsed= </span><span class="si">{</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_start</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> [s] | epoch=</span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1"> | val acc=</span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="c1"># The &quot;patience&quot; logic - should we break out from the training loop? If either validation acc keeps going up</span>
            <span class="c1"># or the val loss keeps going down we won&#39;t stop</span>
            <span class="k">if</span> <span class="n">accuracy</span> <span class="o">&gt;</span> <span class="n">BEST_VAL_ACC</span> <span class="ow">or</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">BEST_VAL_LOSS</span><span class="p">:</span>
                <span class="n">BEST_VAL_ACC</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">BEST_VAL_ACC</span><span class="p">)</span>  <span class="c1"># keep track of the best validation accuracy so far</span>
                <span class="n">BEST_VAL_LOSS</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">BEST_VAL_LOSS</span><span class="p">)</span>
                <span class="n">PATIENCE_CNT</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># reset the counter every time we encounter new best accuracy</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">PATIENCE_CNT</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># otherwise keep counting</span>

            <span class="k">if</span> <span class="n">PATIENCE_CNT</span> <span class="o">&gt;=</span> <span class="n">patience_period</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Stopping the training, the universe has no more patience for this training.&#39;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">accuracy</span>  <span class="c1"># in the case of test phase we just report back the test accuracy</span>

    <span class="k">return</span> <span class="n">main_loop</span>  <span class="c1"># return the decorated function</span>
</pre></div>
</div>
</div>
</div>
<p>That was all we needed! Let’s train it! 💪💪💪</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the graph attention network (GAT)</span>
<span class="n">train_gat</span><span class="p">(</span><span class="n">get_training_args</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GAT training: time elapsed= 0.63 [s] | epoch=1 | val acc=0.126
GAT training: time elapsed= 1.26 [s] | epoch=101 | val acc=0.782
GAT training: time elapsed= 1.84 [s] | epoch=201 | val acc=0.8
GAT training: time elapsed= 2.42 [s] | epoch=301 | val acc=0.784
GAT training: time elapsed= 3.02 [s] | epoch=401 | val acc=0.826
GAT training: time elapsed= 3.60 [s] | epoch=501 | val acc=0.806
GAT training: time elapsed= 4.18 [s] | epoch=601 | val acc=0.8
GAT training: time elapsed= 4.77 [s] | epoch=701 | val acc=0.822
GAT training: time elapsed= 5.43 [s] | epoch=801 | val acc=0.802
GAT training: time elapsed= 6.00 [s] | epoch=901 | val acc=0.79
GAT training: time elapsed= 6.69 [s] | epoch=1001 | val acc=0.79
GAT training: time elapsed= 7.28 [s] | epoch=1101 | val acc=0.784
Stopping the training, the universe has no more patience for this training.
Test accuracy = 0.829
</pre></div>
</div>
</div>
</div>
<p>Nice!!! 🎉🎉🎉 Level 4 unlocked (GAT visualizations 🔮). 😍</p>
<p>We just achieved <code class="docutils literal notranslate"><span class="pre">82.9</span> <span class="pre">%</span></code> on Cora’s test nodes! The same numbers as reported in the original GAT paper!</p>
<p>So we now have everything in place:</p>
<ol class="simple">
<li><p>The data loading and visualizations 📜 -&gt; checked</p></li>
<li><p>GAT model defined 🦄 -&gt; checked</p></li>
<li><p>Training loop setup and the trained model binaries 💪 -&gt; checked</p></li>
</ol>
<p>Now let’s play the GAT model under a microscope 🔬🔬🔬 and understand the weights we got - we can do that in many ways.</p>
</div>
<div class="section" id="visualizing-gat">
<h2>Visualizing GAT<a class="headerlink" href="#visualizing-gat" title="Permalink to this headline">¶</a></h2>
<p>Let’s start by defining some functions we’ll need.</p>
<p>The following cell’s code snippet will get called multiple times so let’s just extract it inside a function - a nice modular design.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gat_forward_pass</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>  <span class="c1"># checking whether you have a GPU, I hope so!</span>

    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;dataset_name&#39;</span><span class="p">:</span> <span class="n">dataset_name</span><span class="p">,</span>
        <span class="s1">&#39;should_visualize&#39;</span><span class="p">:</span> <span class="kc">False</span>  <span class="c1"># don&#39;t visualize the dataset</span>
    <span class="p">}</span>

    <span class="c1"># Step 1: Prepare the data</span>
    <span class="n">node_features</span><span class="p">,</span> <span class="n">node_labels</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_graph_data</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

    <span class="c1"># Step 2: Prepare the model</span>
    <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">BINARIES_PATH</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="n">model_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

    <span class="n">gat</span> <span class="o">=</span> <span class="n">GAT</span><span class="p">(</span>
        <span class="n">num_of_layers</span><span class="o">=</span><span class="n">model_state</span><span class="p">[</span><span class="s1">&#39;num_of_layers&#39;</span><span class="p">],</span>
        <span class="n">num_heads_per_layer</span><span class="o">=</span><span class="n">model_state</span><span class="p">[</span><span class="s1">&#39;num_heads_per_layer&#39;</span><span class="p">],</span>
        <span class="n">num_features_per_layer</span><span class="o">=</span><span class="n">model_state</span><span class="p">[</span><span class="s1">&#39;num_features_per_layer&#39;</span><span class="p">],</span>
        <span class="n">add_skip_connection</span><span class="o">=</span><span class="n">model_state</span><span class="p">[</span><span class="s1">&#39;add_skip_connection&#39;</span><span class="p">],</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">model_state</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">],</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">model_state</span><span class="p">[</span><span class="s1">&#39;dropout&#39;</span><span class="p">],</span>
        <span class="n">log_attention_weights</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">print_model_metadata</span><span class="p">(</span><span class="n">model_state</span><span class="p">)</span>
    <span class="n">gat</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_state</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">],</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">gat</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># some layers like nn.Dropout behave differently in train vs eval mode so this part is important</span>

    <span class="c1"># Step 3: Calculate all the things we&#39;ll need for different visualization types (attention, scores, edge_index)</span>

    <span class="c1"># This context manager is important (and you&#39;ll often see it), otherwise PyTorch will eat much more memory.</span>
    <span class="c1"># It would be saving activations for backprop but we are not going to do any model training just the prediction.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># Step 3: Run predictions and collect the high dimensional data</span>
        <span class="n">all_nodes_unnormalized_scores</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gat</span><span class="p">((</span><span class="n">node_features</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>  <span class="c1"># shape = (N, num of classes)</span>
        <span class="n">all_nodes_unnormalized_scores</span> <span class="o">=</span> <span class="n">all_nodes_unnormalized_scores</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
    <span class="k">return</span> <span class="n">all_nodes_unnormalized_scores</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">node_labels</span><span class="p">,</span> <span class="n">gat</span>
</pre></div>
</div>
</div>
</div>
<p>Nice that one just produces the data that’ll get consumed in the downstream visualizations that you’ll see defined in the following cells.</p>
<p>There’s one more helper function that we’ll need and we’re ready!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Draws (but doesn&#39;t yet plot) the entropy histogram. If you&#39;re confused to why do we have entropy here all of a sudden</span>
<span class="c1"># bear with me you&#39;ll soon understand. Basically it helps us quantify the usefulness of GAT&#39;s learned attention pattern.</span>
<span class="k">def</span> <span class="nf">draw_entropy_histogram</span><span class="p">(</span><span class="n">entropy_array</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">uniform_distribution</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_bins</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">max_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">entropy_array</span><span class="p">)</span>
    <span class="n">bar_width</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_value</span> <span class="o">/</span> <span class="n">num_bins</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">uniform_distribution</span> <span class="k">else</span> <span class="mf">0.75</span><span class="p">)</span>
    <span class="n">histogram_values</span><span class="p">,</span> <span class="n">histogram_bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">entropy_array</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">num_bins</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_value</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">histogram_bins</span><span class="p">[:</span><span class="n">num_bins</span><span class="p">],</span> <span class="n">histogram_values</span><span class="p">[:</span><span class="n">num_bins</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="n">bar_width</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;entropy bins&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;# of node neighborhoods&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Nice, following up is the main function we’ll be using to visualize GAT’s embeddings (via t-SNE) and entropy histograms:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">entropy</span>


<span class="c1"># Let&#39;s define an enum as a clean way to pick between different visualization options</span>
<span class="k">class</span> <span class="nc">VisualizationType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">ATTENTION</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">EMBEDDINGS</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">ENTROPY</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>


<span class="k">def</span> <span class="nf">visualize_gat_properties</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;gat_000000.pth&#39;</span><span class="p">,</span> <span class="n">dataset_name</span><span class="o">=</span><span class="n">DatasetType</span><span class="o">.</span><span class="n">CORA</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">visualization_type</span><span class="o">=</span><span class="n">VisualizationType</span><span class="o">.</span><span class="n">ATTENTION</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pick between visualizing t-SNE or entropy histograms.</span>
<span class="sd">    </span>
<span class="sd">    Notes on t-SNE:</span>
<span class="sd">    Check out this one for more intuition on how to tune t-SNE: https://distill.pub/2016/misread-tsne/</span>

<span class="sd">    If you think it&#39;d be useful for me to implement t-SNE as well and explain how every single detail works</span>
<span class="sd">    open up an issue or DM me on social media! &lt;3</span>

<span class="sd">    Note: I also tried using UMAP but it doesn&#39;t provide any more insight than t-SNE.</span>
<span class="sd">    (con: it has a lot of dependencies if you want to use their plotting functionality)</span>
<span class="sd">    </span>

<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Fetch the data we&#39;ll need to create visualizations</span>
    <span class="n">all_nodes_unnormalized_scores</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">node_labels</span><span class="p">,</span> <span class="n">gat</span> <span class="o">=</span> <span class="n">gat_forward_pass</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">)</span>
    
    <span class="c1"># Perform a specific visualization (t-SNE or entropy histograms)</span>
    <span class="k">if</span> <span class="n">visualization_type</span> <span class="o">==</span> <span class="n">VisualizationType</span><span class="o">.</span><span class="n">EMBEDDINGS</span><span class="p">:</span>  <span class="c1"># visualize embeddings (using t-SNE)</span>
        <span class="n">node_labels</span> <span class="o">=</span> <span class="n">node_labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">node_labels</span><span class="p">))</span>

        <span class="c1"># Feel free to experiment with perplexity, it&#39;s arguable the most important parameter of t-SNE and it basically</span>
        <span class="c1"># controls the standard deviation of Gaussians i.e. the size of the neighborhoods in high dim (original) space.</span>
        <span class="c1"># Simply put the goal of t-SNE is to minimize the KL-divergence between joint Gaussian distribution fit over</span>
        <span class="c1"># high dim points and between the t-Student distribution fit over low dimension points (the ones we&#39;re plotting)</span>
        <span class="c1"># Intuitively, by doing this, we preserve the similarities (relationships) between the high and low dim points.</span>
        <span class="c1"># This (probably) won&#39;t make much sense if you&#39;re not already familiar with t-SNE, God knows I&#39;ve tried. :P</span>
        <span class="n">t_sne_embeddings</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;barnes_hut&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">all_nodes_unnormalized_scores</span><span class="p">)</span>

        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>  <span class="c1"># otherwise plots are really small in Jupyter Notebook</span>
        <span class="k">for</span> <span class="n">class_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
            <span class="c1"># We extract the points whose true label equals class_id and we color them in the same way, hopefully</span>
            <span class="c1"># they&#39;ll be clustered together on the 2D chart - that would mean that GAT has learned good representations!</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">t_sne_embeddings</span><span class="p">[</span><span class="n">node_labels</span> <span class="o">==</span> <span class="n">class_id</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">t_sne_embeddings</span><span class="p">[</span><span class="n">node_labels</span> <span class="o">==</span> <span class="n">class_id</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">cora_label_to_color_map</span><span class="p">[</span><span class="n">class_id</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># We want our local probability distributions (attention weights over the neighborhoods) to be</span>
    <span class="c1"># non-uniform because that means that GAT is learning a useful pattern. Entropy histograms help us visualize</span>
    <span class="c1"># how different those neighborhood distributions are from the uniform distribution (constant attention).</span>
    <span class="c1"># If the GAT is learning const attention we could well be using GCN or some even simpler models.</span>
    <span class="k">elif</span> <span class="n">visualization_type</span> <span class="o">==</span> <span class="n">VisualizationType</span><span class="o">.</span><span class="n">ENTROPY</span><span class="p">:</span>
        <span class="n">num_heads_per_layer</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">num_of_heads</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">gat</span><span class="o">.</span><span class="n">gat_net</span><span class="p">]</span>
        <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_heads_per_layer</span><span class="p">)</span>

        <span class="n">num_of_nodes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">node_features</span><span class="p">)</span>
        <span class="n">target_node_ids</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># For every GAT layer and for every GAT attention head plot the entropy histogram</span>
        <span class="k">for</span> <span class="n">layer_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="c1"># Fetch the attention weights for edges (attention is logged during GAT&#39;s forward pass above)</span>
            <span class="c1"># attention shape = (N, NH, 1) -&gt; (N, NH) - we just squeeze the last dim it&#39;s superfluous</span>
            <span class="n">all_attention_weights</span> <span class="o">=</span> <span class="n">gat</span><span class="o">.</span><span class="n">gat_net</span><span class="p">[</span><span class="n">layer_id</span><span class="p">]</span><span class="o">.</span><span class="n">attention_weights</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">head_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads_per_layer</span><span class="p">[</span><span class="n">layer_id</span><span class="p">]):</span>
                <span class="n">uniform_dist_entropy_list</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># save the ideal uniform histogram as the reference</span>
                <span class="n">neighborhood_entropy_list</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="c1"># This can also be done much more efficiently via scatter_add_ (no for loops)</span>
                <span class="c1"># pseudo: out.scatter_add_(node_dim, -all_attention_weights * log(all_attention_weights), target_index)</span>
                <span class="k">for</span> <span class="n">target_node_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_of_nodes</span><span class="p">):</span>  <span class="c1"># find every the neighborhood for every node in the graph</span>
                    <span class="c1"># These attention weights sum up to 1 by GAT design so we can treat it as a probability distribution</span>
                    <span class="n">neigborhood_attention</span> <span class="o">=</span> <span class="n">all_attention_weights</span><span class="p">[</span><span class="n">target_node_ids</span> <span class="o">==</span> <span class="n">target_node_id</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                    <span class="c1"># Reference uniform distribution of the same length</span>
                    <span class="n">ideal_uniform_attention</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">neigborhood_attention</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">neigborhood_attention</span><span class="p">)</span>

                    <span class="c1"># Calculate the entropy, check out this video if you&#39;re not familiar with the concept:</span>
                    <span class="c1"># https://www.youtube.com/watch?v=ErfnhcEV1O8 (Aurélien Géron)</span>
                    <span class="n">neighborhood_entropy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entropy</span><span class="p">(</span><span class="n">neigborhood_attention</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
                    <span class="n">uniform_dist_entropy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entropy</span><span class="p">(</span><span class="n">ideal_uniform_attention</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

                <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Cora entropy histogram layer=</span><span class="si">{</span><span class="n">layer_id</span><span class="si">}</span><span class="s1">, attention head=</span><span class="si">{</span><span class="n">head_id</span><span class="si">}</span><span class="s1">&#39;</span>
                <span class="n">draw_entropy_histogram</span><span class="p">(</span><span class="n">uniform_dist_entropy_list</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">uniform_distribution</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">draw_entropy_histogram</span><span class="p">(</span><span class="n">neighborhood_entropy_list</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;dodgerblue&#39;</span><span class="p">)</span>

                <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span>  <span class="c1"># get current figure</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
                <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR_PATH</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;layer_</span><span class="si">{</span><span class="n">layer_id</span><span class="si">}</span><span class="s1">_head_</span><span class="si">{</span><span class="n">head_id</span><span class="si">}</span><span class="s1">.jpg&#39;</span><span class="p">))</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Visualization type </span><span class="si">{</span><span class="n">visualization_type</span><span class="si">}</span><span class="s1"> not supported.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Ok! Let us use finally use it! First up - t-SNE.</p>
<div class="section" id="visualizing-gat-s-embeddings-using-t-sne">
<h3>Visualizing GAT’s embeddings using t-SNE<a class="headerlink" href="#visualizing-gat-s-embeddings-using-t-sne" title="Permalink to this headline">¶</a></h3>
<p>t-SNE belongs to a large family of <strong>dimensionality reduction</strong> methods.</p>
<p>It got a huge traction in the community because it’s simple to use and gives nice results (and probably because it was coauthored by Geoffrey Hinton <strong>khm</strong>)</p>
<p>There are other methods like <code class="docutils literal notranslate"><span class="pre">UMAP</span></code> which are newer but haven’t gained that much traction (to the best of my knowledge).</p>
<p>But enough theory let’s see some charts!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;gat_000000.pth&#39;</span>  <span class="c1"># This model is checked-in, feel free to use the one you trained</span>
<span class="n">dataset_name</span><span class="o">=</span><span class="n">DatasetType</span><span class="o">.</span><span class="n">CORA</span><span class="o">.</span><span class="n">name</span>


<span class="n">visualize_gat_properties</span><span class="p">(</span>
        <span class="n">model_name</span><span class="p">,</span>
        <span class="n">dataset_name</span><span class="p">,</span>
        <span class="n">visualization_type</span><span class="o">=</span><span class="n">VisualizationType</span><span class="o">.</span><span class="n">EMBEDDINGS</span>  <span class="c1"># pick between attention, t-SNE embeddings and entropy</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>***** Model training metadata: *****
commit_hash: 91fb864b8f9ddefd401bf5399cea779bd3c0a63b
dataset_name: CORA
num_of_epochs: 10000
test_acc: 0.822
num_of_layers: 2
num_heads_per_layer: [8, 1]
num_features_per_layer: [1433, 8, 7]
add_skip_connection: False
bias: True
dropout: 0.6
layer_type: IMP3
*************************************
</pre></div>
</div>
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_56_1.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_56_1.png" />
</div>
</div>
<p>Beautiful!</p>
<p>What we can see is the following - once we do a forward pass through GAT it transforms the input feature vectors of dimension (number of nodes, number of features per feature vector) = <code class="docutils literal notranslate"><span class="pre">(2708,</span> <span class="pre">1433)</span></code> into <code class="docutils literal notranslate"><span class="pre">(2708,</span> <span class="pre">7)</span></code> because Cora has 7 classes.</p>
<p>The classes are: <code class="docutils literal notranslate"><span class="pre">Genetic</span> <span class="pre">Algorithms</span></code>, <code class="docutils literal notranslate"><span class="pre">Reinforcement</span> <span class="pre">Learning</span></code>, etc. to make it a bit less abstract but ultimately it doesn’t matter it would work for any set of 7 classes (People who subscribe to <a class="reference external" href="https://www.youtube.com/c/TheAIEpiphany">The AI Epiphany</a>, <em>people who.., I’ll stop here</em>).</p>
<p>Now once we’ve got those 7 dimensional vectors we use t-SNE to map them into 2D vectors (<em>because it’s hard to plot 7D vectors you know</em>). The trick with t-SNE is that it preserves the relationships betweeen the vectors so, roughly, if they were close (however we define the “closeness”) in the 7D space they’ll be close in the 2D space as well.</p>
<p>Now you can see that points of the same class (they share the same color) are clustered together! And that’s a desirable property because now it’s much easier to train a classifier that’ll correctly predict the class.</p>
<hr class="docutils" />
<p>Awesome, now let’s switch our focus to attention since we’re dealing with Graph <strong>Attention</strong> Network after all.</p>
</div>
<div class="section" id="visualizing-neighborhood-attention">
<h3>Visualizing neighborhood attention<a class="headerlink" href="#visualizing-neighborhood-attention" title="Permalink to this headline">¶</a></h3>
<p>So, you now hopefully understand how GAT roughly works, and so you know that during the aggregation stage every single node assigns an <strong>attention coefficient</strong> to every single one of its neighbors (including itself since we added self edges).</p>
<p>Any ideas on what we could visualize? Well let’s pick some nodes and see which attention patterns they’ve learned!</p>
<p>The first idea that may pop to your mind is to draw edges <strong>thicker</strong> if the <strong>attention is larger</strong> and vice versa (<em>well that’s also the last idea that pops to my mind</em>).</p>
<p>Let’s do it!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Again, unfortunately, igraph is having some problems running in Jupyter so I have to flatten out the content here</span>
<span class="c1"># including the for loops - no for loops with igraph in Jupyter folks.</span>

<span class="c1"># Fetch the data we&#39;ll need to create visualizations</span>
<span class="n">all_nodes_unnormalized_scores</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">node_labels</span><span class="p">,</span> <span class="n">gat</span> <span class="o">=</span> <span class="n">gat_forward_pass</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">)</span>

<span class="c1"># The number of nodes for which we want to visualize their attention over neighboring nodes</span>
<span class="n">num_nodes_of_interest</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 4 is an arbitrary number you can play with these numbers</span>
<span class="n">head_to_visualize</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># plot attention from this multi-head attention&#39;s head (last layer only has a single head)</span>
<span class="n">gat_layer_id</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># plot attention from this GAT layer (since our GAT only has 2 layers this is the last layer)</span>

<span class="c1"># Build up the complete graph</span>
<span class="c1"># node_features shape = (N, FIN), where N is the number of nodes and FIN number of input features</span>
<span class="n">total_num_of_nodes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">node_features</span><span class="p">)</span>
<span class="n">complete_graph</span> <span class="o">=</span> <span class="n">ig</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">complete_graph</span><span class="o">.</span><span class="n">add_vertices</span><span class="p">(</span><span class="n">total_num_of_nodes</span><span class="p">)</span>  <span class="c1"># igraph creates nodes with ids [0, total_num_of_nodes - 1]</span>
<span class="n">edge_index_tuples</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]))</span>  <span class="c1"># igraph requires this format</span>
<span class="n">complete_graph</span><span class="o">.</span><span class="n">add_edges</span><span class="p">(</span><span class="n">edge_index_tuples</span><span class="p">)</span>

<span class="c1"># Pick the target nodes to plot (nodes with highest degree + random nodes)</span>
<span class="c1"># Note: there could be an overlap between random nodes and nodes with highest degree - but highly unlikely</span>
<span class="n">highest_degree_node_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argpartition</span><span class="p">(</span><span class="n">complete_graph</span><span class="o">.</span><span class="n">degree</span><span class="p">(),</span> <span class="o">-</span><span class="n">num_nodes_of_interest</span><span class="p">)[</span><span class="o">-</span><span class="n">num_nodes_of_interest</span><span class="p">:]</span>
<span class="n">random_node_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">total_num_of_nodes</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">num_nodes_of_interest</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Highest degree nodes = </span><span class="si">{</span><span class="n">highest_degree_node_ids</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
      
<span class="n">target_node_ids</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">source_nodes</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1">#</span>
<span class="c1"># Pick the node id you want to visualize the attention for!</span>
<span class="c1">#</span>

<span class="c1"># since for loops won&#39;t work with igraph just set some number here</span>
<span class="n">target_node_id</span> <span class="o">=</span> <span class="mi">306</span>  <span class="c1"># 306 is the 2nd highest degree node</span>

<span class="c1"># Step 1: Find the neighboring nodes to the target node</span>
<span class="c1"># Note: self edge for CORA is included so the target node is it&#39;s own neighbor (Alexandro yo soy tu madre)</span>
<span class="n">src_nodes_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target_node_ids</span><span class="p">,</span> <span class="n">target_node_id</span><span class="p">)</span>
<span class="n">source_node_ids</span> <span class="o">=</span> <span class="n">source_nodes</span><span class="p">[</span><span class="n">src_nodes_indices</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">size_of_neighborhood</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">source_node_ids</span><span class="p">)</span>

<span class="c1"># Step 2: Fetch their labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">node_labels</span><span class="p">[</span><span class="n">source_node_ids</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># Step 3: Fetch the attention weights for edges (attention is logged during GAT&#39;s forward pass above)</span>
<span class="c1"># attention shape = (N, NH, 1) -&gt; (N, NH) - we just squeeze the last dim it&#39;s superfluous</span>
<span class="n">all_attention_weights</span> <span class="o">=</span> <span class="n">gat</span><span class="o">.</span><span class="n">gat_net</span><span class="p">[</span><span class="n">gat_layer_id</span><span class="p">]</span><span class="o">.</span><span class="n">attention_weights</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">all_attention_weights</span><span class="p">[</span><span class="n">src_nodes_indices</span><span class="p">,</span> <span class="n">head_to_visualize</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="c1"># This part shows that for CORA what GAT learns is pretty much constant attention weights! Like in GCN!</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Max attention weight = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span><span class="si">}</span><span class="s1"> and min = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">attention_weights</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>  <span class="c1"># rescale the biggest weight to 1 for nicer plotting</span>

<span class="c1"># Build up the neighborhood graph whose attention we want to visualize</span>
<span class="c1"># igraph constraint - it works with contiguous range of ids so we map e.g. node 497 to 0, 12 to 1, etc.</span>
<span class="n">id_to_igraph_id</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">source_node_ids</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">source_node_ids</span><span class="p">))))</span>
<span class="n">ig_graph</span> <span class="o">=</span> <span class="n">ig</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">ig_graph</span><span class="o">.</span><span class="n">add_vertices</span><span class="p">(</span><span class="n">size_of_neighborhood</span><span class="p">)</span>
<span class="n">ig_graph</span><span class="o">.</span><span class="n">add_edges</span><span class="p">([(</span><span class="n">id_to_igraph_id</span><span class="p">[</span><span class="n">neighbor</span><span class="p">],</span> <span class="n">id_to_igraph_id</span><span class="p">[</span><span class="n">target_node_id</span><span class="p">])</span> <span class="k">for</span> <span class="n">neighbor</span> <span class="ow">in</span> <span class="n">source_node_ids</span><span class="p">])</span>

<span class="c1"># Prepare the visualization settings dictionary and plot</span>
<span class="n">visual_style</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;edge_width&quot;</span><span class="p">:</span> <span class="n">attention_weights</span><span class="p">,</span>  <span class="c1"># make edges as thick as the corresponding attention weight</span>
    <span class="s2">&quot;layout&quot;</span><span class="p">:</span> <span class="n">ig_graph</span><span class="o">.</span><span class="n">layout_reingold_tilford_circular</span><span class="p">()</span>  <span class="c1"># layout for tree-like graphs</span>
<span class="p">}</span>
<span class="c1"># This is the only part that&#39;s Cora specific as Cora has 7 labels</span>
<span class="k">if</span> <span class="n">dataset_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="n">DatasetType</span><span class="o">.</span><span class="n">CORA</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
    <span class="n">visual_style</span><span class="p">[</span><span class="s2">&quot;vertex_color&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">cora_label_to_color_map</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Add custom color scheme for your specific dataset. Using igraph default coloring.&#39;</span><span class="p">)</span>

<span class="n">ig</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ig_graph</span><span class="p">,</span> <span class="o">**</span><span class="n">visual_style</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>***** Model training metadata: *****
commit_hash: 91fb864b8f9ddefd401bf5399cea779bd3c0a63b
dataset_name: CORA
num_of_epochs: 10000
test_acc: 0.822
num_of_layers: 2
num_heads_per_layer: [8, 1]
num_features_per_layer: [1433, 8, 7]
add_skip_connection: False
bias: True
dropout: 0.6
layer_type: IMP3
*************************************

Highest degree nodes = [1986 1701  306 1358]
Max attention weight = 0.01291587483137846 and min = 0.012394177727401257
</pre></div>
</div>
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_59_1.svg" src="_images/T249662_GAT_in_PyTorch_on_CORA_59_1.svg" /></div>
</div>
<p>Beautiful! 🎉😍</p>
<p>But have you noticed anything, let’s call it, weird? All the edges have the same thickness? What’s up with that?</p>
<p>Well it turns out that on Cora constant attention i.e. the same attention coefficients over the neighborhood does a great job.</p>
<p>On some different graph dataset that is not <code class="docutils literal notranslate"><span class="pre">homophilic</span></code> we’d have more interesting attention patterns - check out the other script for PPI.</p>
<p>But until then, there is one more way to understand whether GAT is learning interesting attention patterns, and that brings us to entropy histograms!</p>
</div>
<div class="section" id="visualizing-entropy-histograms">
<h3>Visualizing entropy histograms<a class="headerlink" href="#visualizing-entropy-histograms" title="Permalink to this headline">¶</a></h3>
<p>So, I hear you say, wait <code class="docutils literal notranslate"><span class="pre">entropy</span></code>, what? How did <a class="reference external" href="http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">Claude Shannon</a> find his way in?</p>
<p>Well it’s not that hard. The attention coefficients sum up to 1 - they form a probability distribution. Where there is a probability distribution you can calculate the entropy. And the entropy quantifies the amount of information in a distribution (for my uber geeks - it’s an expected value of self-information 🤓).</p>
<p>Check out this <a class="reference external" href="https://www.youtube.com/watch?v=ErfnhcEV1O8">amazing video</a> if you’re not familiar with the concept of entropy, but actually you don’t need to understand the theory of entropy so much in order to understand why we’re doing this.</p>
<p>The main idea is the following:</p>
<p>If we have a <strong>“hypothetical” GAT</strong> that has a const attention over every node’s neighborhood (i.e. <strong>all distributions are uniform</strong>), and we calculate the entropy (whatever that may be) of each and every neighborhood, and we make a histogram out of those numbers - <strong>how different are the histograms</strong> coming from it compared to the GAT we just trained?</p>
<p>If the answer is they completely overlap, well that means our GAT has got uniform attention patterns. The smaller the overlap the less uniform the distributions are. We don’t care about the information, per se, we care about how much the histograms <strong>match</strong>.</p>
<p>Helpfully that brings some clarity into your mind. With that out of the way let’s visualize the damn thing:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_gat_properties</span><span class="p">(</span>
        <span class="n">model_name</span><span class="p">,</span>
        <span class="n">dataset_name</span><span class="p">,</span>
        <span class="n">visualization_type</span><span class="o">=</span><span class="n">VisualizationType</span><span class="o">.</span><span class="n">ENTROPY</span>  <span class="c1"># pick between attention, t-SNE embeddings and entropy</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>***** Model training metadata: *****
commit_hash: 91fb864b8f9ddefd401bf5399cea779bd3c0a63b
dataset_name: CORA
num_of_epochs: 10000
test_acc: 0.822
num_of_layers: 2
num_heads_per_layer: [8, 1]
num_features_per_layer: [1433, 8, 7]
add_skip_connection: False
bias: True
dropout: 0.6
layer_type: IMP3
*************************************
</pre></div>
</div>
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_62_1.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_62_1.png" />
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_62_2.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_62_2.png" />
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_62_3.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_62_3.png" />
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_62_4.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_62_4.png" />
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_62_5.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_62_5.png" />
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_62_6.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_62_6.png" />
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_62_7.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_62_7.png" />
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_62_8.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_62_8.png" />
<img alt="_images/T249662_GAT_in_PyTorch_on_CORA_62_9.png" src="_images/T249662_GAT_in_PyTorch_on_CORA_62_9.png" />
</div>
</div>
<p>And voilà, the light blue histograms (trained GAT) completely match the orange ones (uniform attention GAT).</p>
<p>If the previous visualization with edge thickness plotted didn’t convince you I’m sure that entropy will! (<em>laughs in kilo bits per cringe</em>)</p>
<p>The idea for this visualization came from <a class="reference external" href="https://www.dgl.ai/blog/2019/02/17/gat.html">this blog post</a> recommended to me by Petar Veličković.</p>
<hr class="docutils" />
<p>Phew!!! That was a mouthful! If you stayed with me until here, <strong>congrats!</strong> (achievement unlocked - GAT master 😍)</p>
<p>Take your time to analyze this notebook. This is not a toy project, it took me ~3 weeks to finish it <br/>
so don’t expect to understand everything in 30 minutes unless you’re really familiar with most of the concepts mentioned here.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sparsh-ai/graph-embeddings",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T874868_GAT_in_Tensorflow.x.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">GAT using Tensorflow</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T336765_FB15K_Graph_Embedding_Learning_in_PyTorch_Big_Graph.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">FB15K Graph Embedding Learning in PyTorch Big Graph</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>